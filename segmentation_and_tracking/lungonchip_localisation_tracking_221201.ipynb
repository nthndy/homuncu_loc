{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0d243d9a",
   "metadata": {},
   "source": [
    "# LoC image analysis\n",
    "\n",
    "This notebook is designed to take the raw output of a LoC data set, monolayer or bilayer, and:\n",
    " (trying to check if dask multichan works)\n",
    "\n",
    "5. Localise and measure properties\n",
    "6. Unite the localisations over the z-range\n",
    "7. Save out the Z-tracks\n",
    "8. Extract the maximum intensity from each cell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b3341d38",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/dayn/miniconda3/envs/btrack_test/lib/python3.9/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import glob\n",
    "from octopusheavy import DaskOctopusHeavyLoader\n",
    "import napari\n",
    "from skimage.io import imshow,  imsave, imread\n",
    "import napari\n",
    "import btrack\n",
    "from tqdm.auto import tqdm\n",
    "import dask.array as da"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce96b2e7",
   "metadata": {},
   "source": [
    "# Loading images\n",
    "\n",
    "Define root path and individual experiment IDs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f17b38de",
   "metadata": {},
   "outputs": [],
   "source": [
    "root_path = '/run/user/30046150/gvfs/smb-share:server=data.thecrick.org,share=lab-gutierrezm/home/shared/Lung on chip/Light microscopy'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "577b6085",
   "metadata": {},
   "outputs": [],
   "source": [
    "expt_IDs = ['co-culture/iVECs+iAT2AT1/Folder_20220808/A2-A5/analysis_20221125/DAPI-SPC-PDPN-ZO1/_20220808_kolf-WT_co-culture_20x_A2-A5_Multichannel Z-Stack_20220808_60_/images/',\n",
    "            'co-culture/iVECs+iAT2AT1/Folder_20220808/A2-A5/analysis_20221125/DAPI-VWF-iCAM1-ZO1/_20220808_kolf-WT_co-culture_20x_A2-A5_Multichannel Z-Stack_20220808_70_/images/',\n",
    "            'mono-culture/iAT2AT1/for analysis_20221125/DAPI-AQP5-proSPC-ZO1/Day7_static/_20x_21-12-031B_A12456_Multichannel Z-Stack_20220811_150_/images/',\n",
    "            'mono-culture/iAT2AT1/for analysis_20221125/DAPI-AQP5-proSPC-ZO1/Day14_static/_20x_21-12-028A_A23456_Multichannel Z-Stack_20220818_262_/images/',\n",
    "            'mono-culture/iAT2AT1/for analysis_20221125/DAPI-CAV1-proSPC-ZO1/Day7_static/_20x_21-12-031B_A12456_Multichannel Z-Stack_20220811_130_/images/',\n",
    "            'mono-culture/iAT2AT1/for analysis_20221125/DAPI-CAV1-proSPC-ZO1/Day14_static/_20x_21-12-028A_A23456_Multichannel Z-Stack_20220818_253_/images/'\n",
    "           ]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9560206",
   "metadata": {},
   "source": [
    "## Check to see what channels each expt has"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b51972e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 [<Channels.CH1: 1>, <Channels.CH2: 2>, <Channels.CH3: 3>, <Channels.CH4: 4>, <Channels.MASK1: 99>]\n",
      "1 [<Channels.CH1: 1>, <Channels.CH2: 2>, <Channels.CH3: 3>, <Channels.CH4: 4>, <Channels.MASK1: 99>]\n",
      "2 [<Channels.CH1: 1>, <Channels.CH2: 2>, <Channels.CH3: 3>, <Channels.CH4: 4>, <Channels.MASK1: 99>]\n",
      "3 [<Channels.CH1: 1>, <Channels.CH2: 2>, <Channels.CH3: 3>, <Channels.CH4: 4>, <Channels.MASK1: 99>]\n",
      "4 [<Channels.CH1: 1>, <Channels.CH2: 2>, <Channels.CH3: 3>, <Channels.CH4: 4>, <Channels.MASK1: 99>]\n",
      "5 [<Channels.CH1: 1>, <Channels.CH2: 2>, <Channels.CH3: 3>, <Channels.CH4: 4>, <Channels.MASK1: 99>]\n"
     ]
    }
   ],
   "source": [
    "for i, expt in enumerate(expt_IDs):\n",
    "    images = DaskOctopusHeavyLoader(os.path.join(root_path, expt), remove_background=False)\n",
    "    if 'MASK1' in [channel.name for channel in images.channels]:\n",
    "        images = DaskOctopusHeavyLoader(os.path.join(root_path, expt), remove_background=False)\n",
    "        print(i, images.channels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e2132fa2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table>\n",
       "    <tr>\n",
       "        <td>\n",
       "            <table style=\"border-collapse: collapse;\">\n",
       "                <thead>\n",
       "                    <tr>\n",
       "                        <td> </td>\n",
       "                        <th> Array </th>\n",
       "                        <th> Chunk </th>\n",
       "                    </tr>\n",
       "                </thead>\n",
       "                <tbody>\n",
       "                    \n",
       "                    <tr>\n",
       "                        <th> Bytes </th>\n",
       "                        <td> 820.12 MiB </td>\n",
       "                        <td> 10.12 MiB </td>\n",
       "                    </tr>\n",
       "                    \n",
       "                    <tr>\n",
       "                        <th> Shape </th>\n",
       "                        <td> (81, 2304, 2304) </td>\n",
       "                        <td> (1, 2304, 2304) </td>\n",
       "                    </tr>\n",
       "                    <tr>\n",
       "                        <th> Dask graph </th>\n",
       "                        <td colspan=\"2\"> 81 chunks in 163 graph layers </td>\n",
       "                    </tr>\n",
       "                    <tr>\n",
       "                        <th> Data type </th>\n",
       "                        <td colspan=\"2\"> uint16 numpy.ndarray </td>\n",
       "                    </tr>\n",
       "                </tbody>\n",
       "            </table>\n",
       "        </td>\n",
       "        <td>\n",
       "        <svg width=\"199\" height=\"189\" style=\"stroke:rgb(0,0,0);stroke-width:1\" >\n",
       "\n",
       "  <!-- Horizontal lines -->\n",
       "  <line x1=\"10\" y1=\"0\" x2=\"29\" y2=\"19\" style=\"stroke-width:2\" />\n",
       "  <line x1=\"10\" y1=\"120\" x2=\"29\" y2=\"139\" style=\"stroke-width:2\" />\n",
       "\n",
       "  <!-- Vertical lines -->\n",
       "  <line x1=\"10\" y1=\"0\" x2=\"10\" y2=\"120\" style=\"stroke-width:2\" />\n",
       "  <line x1=\"10\" y1=\"0\" x2=\"10\" y2=\"120\" />\n",
       "  <line x1=\"11\" y1=\"1\" x2=\"11\" y2=\"121\" />\n",
       "  <line x1=\"12\" y1=\"2\" x2=\"12\" y2=\"122\" />\n",
       "  <line x1=\"13\" y1=\"3\" x2=\"13\" y2=\"123\" />\n",
       "  <line x1=\"14\" y1=\"4\" x2=\"14\" y2=\"124\" />\n",
       "  <line x1=\"15\" y1=\"5\" x2=\"15\" y2=\"125\" />\n",
       "  <line x1=\"16\" y1=\"6\" x2=\"16\" y2=\"126\" />\n",
       "  <line x1=\"17\" y1=\"7\" x2=\"17\" y2=\"127\" />\n",
       "  <line x1=\"18\" y1=\"8\" x2=\"18\" y2=\"128\" />\n",
       "  <line x1=\"19\" y1=\"9\" x2=\"19\" y2=\"129\" />\n",
       "  <line x1=\"20\" y1=\"10\" x2=\"20\" y2=\"130\" />\n",
       "  <line x1=\"21\" y1=\"11\" x2=\"21\" y2=\"131\" />\n",
       "  <line x1=\"22\" y1=\"12\" x2=\"22\" y2=\"132\" />\n",
       "  <line x1=\"23\" y1=\"13\" x2=\"23\" y2=\"133\" />\n",
       "  <line x1=\"24\" y1=\"14\" x2=\"24\" y2=\"134\" />\n",
       "  <line x1=\"25\" y1=\"15\" x2=\"25\" y2=\"135\" />\n",
       "  <line x1=\"26\" y1=\"16\" x2=\"26\" y2=\"136\" />\n",
       "  <line x1=\"27\" y1=\"17\" x2=\"27\" y2=\"137\" />\n",
       "  <line x1=\"29\" y1=\"19\" x2=\"29\" y2=\"139\" style=\"stroke-width:2\" />\n",
       "\n",
       "  <!-- Colored Rectangle -->\n",
       "  <polygon points=\"10.0,0.0 29.02765395748794,19.02765395748794 29.02765395748794,139.02765395748793 10.0,120.0\" style=\"fill:#8B4903A0;stroke-width:0\"/>\n",
       "\n",
       "  <!-- Horizontal lines -->\n",
       "  <line x1=\"10\" y1=\"0\" x2=\"130\" y2=\"0\" style=\"stroke-width:2\" />\n",
       "  <line x1=\"10\" y1=\"0\" x2=\"130\" y2=\"0\" />\n",
       "  <line x1=\"11\" y1=\"1\" x2=\"131\" y2=\"1\" />\n",
       "  <line x1=\"12\" y1=\"2\" x2=\"132\" y2=\"2\" />\n",
       "  <line x1=\"13\" y1=\"3\" x2=\"133\" y2=\"3\" />\n",
       "  <line x1=\"14\" y1=\"4\" x2=\"134\" y2=\"4\" />\n",
       "  <line x1=\"15\" y1=\"5\" x2=\"135\" y2=\"5\" />\n",
       "  <line x1=\"16\" y1=\"6\" x2=\"136\" y2=\"6\" />\n",
       "  <line x1=\"17\" y1=\"7\" x2=\"137\" y2=\"7\" />\n",
       "  <line x1=\"18\" y1=\"8\" x2=\"138\" y2=\"8\" />\n",
       "  <line x1=\"19\" y1=\"9\" x2=\"139\" y2=\"9\" />\n",
       "  <line x1=\"20\" y1=\"10\" x2=\"140\" y2=\"10\" />\n",
       "  <line x1=\"21\" y1=\"11\" x2=\"141\" y2=\"11\" />\n",
       "  <line x1=\"22\" y1=\"12\" x2=\"142\" y2=\"12\" />\n",
       "  <line x1=\"23\" y1=\"13\" x2=\"143\" y2=\"13\" />\n",
       "  <line x1=\"24\" y1=\"14\" x2=\"144\" y2=\"14\" />\n",
       "  <line x1=\"25\" y1=\"15\" x2=\"145\" y2=\"15\" />\n",
       "  <line x1=\"26\" y1=\"16\" x2=\"146\" y2=\"16\" />\n",
       "  <line x1=\"27\" y1=\"17\" x2=\"147\" y2=\"17\" />\n",
       "  <line x1=\"29\" y1=\"19\" x2=\"149\" y2=\"19\" style=\"stroke-width:2\" />\n",
       "\n",
       "  <!-- Vertical lines -->\n",
       "  <line x1=\"10\" y1=\"0\" x2=\"29\" y2=\"19\" style=\"stroke-width:2\" />\n",
       "  <line x1=\"130\" y1=\"0\" x2=\"149\" y2=\"19\" style=\"stroke-width:2\" />\n",
       "\n",
       "  <!-- Colored Rectangle -->\n",
       "  <polygon points=\"10.0,0.0 130.0,0.0 149.02765395748793,19.02765395748794 29.02765395748794,19.02765395748794\" style=\"fill:#8B4903A0;stroke-width:0\"/>\n",
       "\n",
       "  <!-- Horizontal lines -->\n",
       "  <line x1=\"29\" y1=\"19\" x2=\"149\" y2=\"19\" style=\"stroke-width:2\" />\n",
       "  <line x1=\"29\" y1=\"139\" x2=\"149\" y2=\"139\" style=\"stroke-width:2\" />\n",
       "\n",
       "  <!-- Vertical lines -->\n",
       "  <line x1=\"29\" y1=\"19\" x2=\"29\" y2=\"139\" style=\"stroke-width:2\" />\n",
       "  <line x1=\"149\" y1=\"19\" x2=\"149\" y2=\"139\" style=\"stroke-width:2\" />\n",
       "\n",
       "  <!-- Colored Rectangle -->\n",
       "  <polygon points=\"29.02765395748794,19.02765395748794 149.02765395748793,19.02765395748794 149.02765395748793,139.02765395748793 29.02765395748794,139.02765395748793\" style=\"fill:#ECB172A0;stroke-width:0\"/>\n",
       "\n",
       "  <!-- Text -->\n",
       "  <text x=\"89.027654\" y=\"159.027654\" font-size=\"1.0rem\" font-weight=\"100\" text-anchor=\"middle\" >2304</text>\n",
       "  <text x=\"169.027654\" y=\"79.027654\" font-size=\"1.0rem\" font-weight=\"100\" text-anchor=\"middle\" transform=\"rotate(-90,169.027654,79.027654)\">2304</text>\n",
       "  <text x=\"9.513827\" y=\"149.513827\" font-size=\"1.0rem\" font-weight=\"100\" text-anchor=\"middle\" transform=\"rotate(45,9.513827,149.513827)\">81</text>\n",
       "</svg>\n",
       "        </td>\n",
       "    </tr>\n",
       "</table>"
      ],
      "text/plain": [
       "dask.array<stack, shape=(81, 2304, 2304), dtype=uint16, chunksize=(1, 2304, 2304), chunktype=numpy.ndarray>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "images['CH1']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b7d03a9",
   "metadata": {},
   "source": [
    "### Defining properties to measure similarities of z-slices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "841334ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "props = ('axis_major_length', \n",
    "         'axis_minor_length', \n",
    "         'eccentricity', \n",
    "         'area', \n",
    "         #'intensity_image', \n",
    "         'orientation',\n",
    "         'mean_intensity')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d8be8da",
   "metadata": {},
   "source": [
    "# I think if this one doesn't work it's due to the filename... but surely because I'm reading images from that address then the filepath works...?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c8a48401",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                            | 0/6 [00:00<?, ?it/s][INFO][2023/01/10 10:33:48 AM] Localizing objects from segmentation...\n",
      "[INFO][2023/01/10 10:33:48 AM] Found intensity_image data\n",
      "[INFO][2023/01/10 10:35:57 AM] Objects are of type: <class 'dict'>\n",
      "[INFO][2023/01/10 10:35:57 AM] ...Found 30362 objects in 68 frames.\n",
      "[INFO][2023/01/10 10:35:57 AM] Loaded btrack: /home/dayn/analysis/BayesianTracker_/btrack/libs/libtracker.so\n",
      "[INFO][2023/01/10 10:35:57 AM] btrack (v0.5.0) library imported\n",
      "[INFO][2023/01/10 10:35:57 AM] Starting BayesianTracker session\n",
      "[INFO][2023/01/10 10:35:57 AM] Loading configuration file: /home/dayn/analysis/BayesianTracker/models/particle_config.json\n",
      "[INFO][2023/01/10 10:35:57 AM] Setting max_search_radius -> 100\n",
      "[INFO][2023/01/10 10:35:57 AM] Setting features -> ('axis_major_length', 'axis_minor_length', 'eccentricity', 'area', 'orientation', 'mean_intensity-0', 'mean_intensity-1', 'mean_intensity-2', 'mean_intensity-3')\n",
      "[INFO][2023/01/10 10:35:57 AM] Objects are of type: <class 'list'>\n",
      "[INFO][2023/01/10 10:35:58 AM] Setting volume -> ((0, 2304), (0, 2304), (-100000.0, 100000.0))\n",
      "[INFO][2023/01/10 10:35:58 AM] Starting tracking... \n",
      "[INFO][2023/01/10 10:35:58 AM] Update using: ['MOTION', 'VISUAL']\n",
      "[INFO][2023/01/10 10:35:58 AM] Tracking objects in frames 0 to 9 (of 68)...\n",
      "[INFO][2023/01/10 10:35:59 AM]  - Timing (Bayesian updates: 144.03ms, Linking: 3.60ms)\n",
      "[INFO][2023/01/10 10:35:59 AM]  - Probabilities (Link: 1.00000, Lost: 1.00000)\n",
      "[INFO][2023/01/10 10:35:59 AM]  - Stats (Active: 581, Lost: 147, Conflicts resolved: 210)\n",
      "[INFO][2023/01/10 10:35:59 AM] Tracking objects in frames 10 to 19 (of 68)...\n",
      "[INFO][2023/01/10 10:36:29 AM]  - Timing (Bayesian updates: 3215.99ms, Linking: 20.52ms)\n",
      "[INFO][2023/01/10 10:36:29 AM]  - Probabilities (Link: 1.00000, Lost: 1.00000)\n",
      "[INFO][2023/01/10 10:36:29 AM]  - Stats (Active: 2385, Lost: 2064, Conflicts resolved: 1282)\n",
      "[INFO][2023/01/10 10:36:29 AM] Tracking objects in frames 20 to 29 (of 68)...\n",
      "[INFO][2023/01/10 10:37:12 AM]  - Timing (Bayesian updates: 164.67ms, Linking: 7.38ms)\n",
      "[INFO][2023/01/10 10:37:12 AM]  - Probabilities (Link: 0.99994, Lost: 1.00000)\n",
      "[INFO][2023/01/10 10:37:12 AM]  - Stats (Active: 1996, Lost: 9622, Conflicts resolved: 3932)\n",
      "[INFO][2023/01/10 10:37:12 AM] Tracking objects in frames 30 to 39 (of 68)...\n",
      "[INFO][2023/01/10 10:37:12 AM]  - Timing (Bayesian updates: 10.68ms, Linking: 1.16ms)\n",
      "[INFO][2023/01/10 10:37:12 AM]  - Probabilities (Link: 1.00000, Lost: 1.00000)\n",
      "[INFO][2023/01/10 10:37:12 AM]  - Stats (Active: 401, Lost: 16144, Conflicts resolved: 4344)\n",
      "[INFO][2023/01/10 10:37:12 AM] Tracking objects in frames 40 to 49 (of 68)...\n",
      "[INFO][2023/01/10 10:37:13 AM]  - Timing (Bayesian updates: 0.03ms, Linking: 0.16ms)\n",
      "[INFO][2023/01/10 10:37:13 AM]  - Probabilities (Link: 1.00000, Lost: 1.00000)\n",
      "[INFO][2023/01/10 10:37:13 AM]  - Stats (Active: 94, Lost: 18283, Conflicts resolved: 4518)\n",
      "[INFO][2023/01/10 10:37:13 AM] Tracking objects in frames 50 to 59 (of 68)...\n",
      "[INFO][2023/01/10 10:37:13 AM]  - Timing (Bayesian updates: 0.00ms, Linking: 0.02ms)\n",
      "[INFO][2023/01/10 10:37:13 AM]  - Probabilities (Link: 1.00000, Lost: 1.00000)\n",
      "[INFO][2023/01/10 10:37:13 AM]  - Stats (Active: 10, Lost: 18494, Conflicts resolved: 4526)\n",
      "[INFO][2023/01/10 10:37:13 AM] Tracking objects in frames 60 to 68 (of 68)...\n",
      "[INFO][2023/01/10 10:37:13 AM]  - Timing (Bayesian updates: 0.00ms, Linking: 0.01ms)\n",
      "[INFO][2023/01/10 10:37:13 AM]  - Probabilities (Link: 1.00000, Lost: 1.00000)\n",
      "[INFO][2023/01/10 10:37:13 AM] SUCCESS.\n",
      "[INFO][2023/01/10 10:37:13 AM]  - Found 4053 tracks in 68 frames (in 0.0s)\n",
      "[INFO][2023/01/10 10:37:13 AM]  - Inserted 4025 dummy objects to fill tracking gaps\n",
      "[INFO][2023/01/10 10:37:13 AM] Loading hypothesis model: particle_hypothesis\n",
      "[INFO][2023/01/10 10:37:13 AM] Calculating hypotheses (relax: True)...\n",
      "[INFO][2023/01/10 10:37:13 AM] Setting up constraints matrix for global optimisation...\n",
      "[INFO][2023/01/10 10:37:13 AM] Using GLPK options: {'tm_lim': 60000}...\n",
      "[INFO][2023/01/10 10:37:13 AM] Optimizing...\n",
      "[INFO][2023/01/10 10:37:13 AM] Optimization complete. (Solution: optimal)\n",
      "[INFO][2023/01/10 10:37:13 AM]  - Fates.FALSE_POSITIVE: 1031 (of 4053)\n",
      "[INFO][2023/01/10 10:37:13 AM]  - Fates.LINK: 251 (of 441)\n",
      "[INFO][2023/01/10 10:37:13 AM]  - Fates.INITIALIZE_BORDER: 108 (of 155)\n",
      "[INFO][2023/01/10 10:37:13 AM]  - Fates.INITIALIZE_FRONT: 62 (of 66)\n",
      "[INFO][2023/01/10 10:37:13 AM]  - Fates.INITIALIZE_LAZY: 2601 (of 3832)\n",
      "[INFO][2023/01/10 10:37:13 AM]  - Fates.TERMINATE_BORDER: 101 (of 147)\n",
      "[INFO][2023/01/10 10:37:13 AM]  - Fates.TERMINATE_BACK: 1 (of 1)\n",
      "[INFO][2023/01/10 10:37:13 AM]  - Fates.TERMINATE_LAZY: 2669 (of 3905)\n",
      "[INFO][2023/01/10 10:37:13 AM]  - TOTAL: 12600 hypotheses\n",
      "[INFO][2023/01/10 10:37:13 AM] Completed optimization with 3802 tracks\n",
      "[WARNING][2023/01/10 10:37:13 AM] Changing HDF filename to co-culture_iVECs+iAT2AT1_Folder_20220808_A2-A5_analysis_20221125_DAPI-SPC-PDPN-ZO1__20220808_kolf-WT_co-culture_20x_A2-A5_Multichannel Z-Stack_20220808_60__tracks.h5\n",
      "[INFO][2023/01/10 10:37:13 AM] Opening HDF file: co-culture_iVECs+iAT2AT1_Folder_20220808_A2-A5_analysis_20221125_DAPI-SPC-PDPN-ZO1__20220808_kolf-WT_co-culture_20x_A2-A5_Multichannel Z-Stack_20220808_60__tracks.h5...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GLPK Integer Optimizer 5.0\n",
      "16212 rows, 12600 columns, 17094 non-zeros\n",
      "12600 integer variables, all of which are binary\n",
      "Preprocessing...\n",
      "8106 rows, 12600 columns, 17094 non-zeros\n",
      "12600 integer variables, all of which are binary\n",
      "Scaling...\n",
      " A: min|aij| =  1.000e+00  max|aij| =  1.000e+00  ratio =  1.000e+00\n",
      "Problem data seem to be well scaled\n",
      "Constructing initial basis...\n",
      "Size of triangular part is 8106\n",
      "Solving LP relaxation...\n",
      "GLPK Simplex Optimizer 5.0\n",
      "8106 rows, 12600 columns, 17094 non-zeros\n",
      "*     0: obj =   5.291086383e+04 inf =   0.000e+00 (1665)\n",
      "Perturbing LP to avoid stalling [741]...\n",
      "Removing LP perturbation [1645]...\n",
      "*  1645: obj =   4.357230139e+04 inf =   0.000e+00 (0)\n",
      "OPTIMAL LP SOLUTION FOUND\n",
      "Integer optimization begins...\n",
      "Long-step dual simplex will be used\n",
      "+  1645: mip =     not found yet >=              -inf        (1; 0)\n",
      "+  1645: >>>>>   4.357230139e+04 >=   4.357230139e+04   0.0% (1; 0)\n",
      "+  1645: mip =   4.357230139e+04 >=     tree is empty   0.0% (0; 1)\n",
      "INTEGER OPTIMAL SOLUTION FOUND\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO][2023/01/10 10:37:14 AM] Writing tracks/obj_type_1\n",
      "[WARNING][2023/01/10 10:37:14 AM] Removing tracks/obj_type_1.\n",
      "[INFO][2023/01/10 10:37:14 AM] Writing dummies/obj_type_1\n",
      "[INFO][2023/01/10 10:37:14 AM] Writing LBEP/obj_type_1\n",
      "[INFO][2023/01/10 10:37:14 AM] Writing fates/obj_type_1\n",
      "[INFO][2023/01/10 10:37:14 AM] Closing HDF file: co-culture_iVECs+iAT2AT1_Folder_20220808_A2-A5_analysis_20221125_DAPI-SPC-PDPN-ZO1__20220808_kolf-WT_co-culture_20x_A2-A5_Multichannel Z-Stack_20220808_60__tracks.h5\n",
      "[INFO][2023/01/10 10:37:14 AM] Ending BayesianTracker session\n",
      "  0%|                                                                            | 0/6 [03:26<?, ?it/s]\n"
     ]
    }
   ],
   "source": [
    "for i, expt in tqdm(enumerate(expt_IDs), total = len(expt_IDs)):\n",
    "    props = ('axis_major_length', \n",
    "         'axis_minor_length', \n",
    "         'eccentricity', \n",
    "         'area', \n",
    "         #'intensity_image', \n",
    "         'orientation',\n",
    "         'mean_intensity')\n",
    "    images = DaskOctopusHeavyLoader(os.path.join(root_path, expt), remove_background=False)\n",
    "    multichannel_stack = da.stack([images['CH1'], images['CH2'], images['CH3'], images['CH4']], axis = -1)\n",
    "    if 'MASK1' in [channel.name for channel in images.channels]: \n",
    "        ### create filename for tracks and objects \n",
    "        dirname = os.path.dirname(images.files('CH1')[0]).replace('images','')\n",
    "        objects_fn = os.path.join(dirname, 'objects.hdf5')\n",
    "        tracks_fn = os.path.join(dirname, 'tracks.hdf5')\n",
    "        ### try saving locally instead of online\n",
    "        tracks_fn = tracks_fn.split('Light microscopy/')[-1].replace('/', '_')\n",
    "        objects = btrack.utils.segmentation_to_objects(\n",
    "            images['MASK1'], \n",
    "            multichannel_stack,\n",
    "            properties = props,#('area', 'mean_intensity', 'intensity_image'), \n",
    "            use_weighted_centroid = False\n",
    "        )\n",
    "        ### prune objects \n",
    "        objects = [o for o in objects if o.properties['area']>50]\n",
    "#         ### save out objects\n",
    "#         with btrack.dataio.HDF5FileHandler(\n",
    "#              objects_fn, 'w', obj_type='obj_type_1',\n",
    "#         ) as hdf:\n",
    "#             hdf.write_segmentation(images['MASK1'])\n",
    "#             hdf.write_objects(objects)\n",
    "        ### redefine properties as multichannel image was measured\n",
    "        props = ('axis_major_length', \n",
    "             'axis_minor_length', \n",
    "             'eccentricity', \n",
    "             'area', \n",
    "             #'intensity_image', \n",
    "             'orientation',\n",
    "             'mean_intensity-0',\n",
    "             'mean_intensity-1',\n",
    "             'mean_intensity-2',\n",
    "             'mean_intensity-3',)\n",
    "        \n",
    "        # initialise a tracker session using a context manager\n",
    "        with btrack.BayesianTracker() as tracker:\n",
    "            # configure the tracker using a config file\n",
    "            tracker.configure('/home/dayn/analysis/BayesianTracker/models/particle_config.json')\n",
    "            tracker.verbose = True\n",
    "            ### set max search radius\n",
    "            tracker.max_search_radius = 100\n",
    "            # use visual features to track\n",
    "            tracker.features = props\n",
    "            # append the objects to be tracked\n",
    "            tracker.append(objects)\n",
    "            # set the volume (Z axis volume limits default to [-1e5, 1e5] for 2D data)\n",
    "            tracker.volume=((0, 2304), (0, 2304), (-1e5, 1e5))\n",
    "            # track them (in interactive mode)\n",
    "#             tracker.track_interactive(step_size=100)\n",
    "            tracker.track(tracking_updates =['visual', 'motion'], step_size=10)\n",
    "            # generate hypotheses and run the global optimizer\n",
    "            tracker.optimize()\n",
    "            # get the tracks as a python list\n",
    "            tracks = tracker.tracks\n",
    "#             # filter tracks\n",
    "#             tracks = [track for track in tracks if len(track) >= 3]\n",
    "            # optional: get the data in a format for napari\n",
    "            #   data, properties, graph = tracker.to_napari()\n",
    "            tracker.export(tracks_fn, obj_type = 'obj_type_1')\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "af367fa6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>x</th>\n",
       "      <th>y</th>\n",
       "      <th>z</th>\n",
       "      <th>t</th>\n",
       "      <th>dummy</th>\n",
       "      <th>states</th>\n",
       "      <th>label</th>\n",
       "      <th>axis_major_length</th>\n",
       "      <th>axis_minor_length</th>\n",
       "      <th>eccentricity</th>\n",
       "      <th>area</th>\n",
       "      <th>orientation</th>\n",
       "      <th>mean_intensity-0</th>\n",
       "      <th>mean_intensity-1</th>\n",
       "      <th>mean_intensity-2</th>\n",
       "      <th>mean_intensity-3</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>388.922414</td>\n",
       "      <td>3.62931</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td>7</td>\n",
       "      <td>5</td>\n",
       "      <td>21.899223</td>\n",
       "      <td>6.872596</td>\n",
       "      <td>0.94948</td>\n",
       "      <td>116</td>\n",
       "      <td>-1.485736</td>\n",
       "      <td>101.060345</td>\n",
       "      <td>106.413793</td>\n",
       "      <td>106.784483</td>\n",
       "      <td>111.672414</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "{'ID': 0, 'x': 388.92241379310343, 'y': 3.6293103448275863, 'z': 0.0, 't': 0, 'dummy': False, 'states': 7, 'label': 5, 'axis_major_length': 21.899223324295903, 'axis_minor_length': 6.872596236447866, 'eccentricity': 0.9494797481365117, 'area': 116, 'orientation': -1.4857361042535562, 'mean_intensity-0': 101.0603448275862, 'mean_intensity-1': 106.41379310344827, 'mean_intensity-2': 106.78448275862068, 'mean_intensity-3': 111.67241379310344}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "objects[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "559bbb90",
   "metadata": {},
   "source": [
    "# It won't let me save tracks to the server? But will let me save locally"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d29f678a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'co-culture_iVECs+iAT2AT1_Folder_20220808_A2-A5_analysis_20221125_DAPI-SPC-PDPN-ZO1__20220808_kolf-WT_co-culture_20x_A2-A5_Multichannel Z-Stack_20220808_60__tracks.hdf5'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tracks_fn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d005e395",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO][2023/01/10 10:37:47 AM] Opening HDF file: tracks.h5...\n",
      "[INFO][2023/01/10 10:37:47 AM] Writing objects/obj_type_1\n",
      "[INFO][2023/01/10 10:37:47 AM] Writing labels/obj_type_1\n",
      "[INFO][2023/01/10 10:37:47 AM] Loading objects/obj_type_1 (29045, 5) (29045 filtered: None)\n",
      "[INFO][2023/01/10 10:37:47 AM] Writing properties/obj_type_1/axis_major_length (29045,)\n",
      "[INFO][2023/01/10 10:37:47 AM] Writing properties/obj_type_1/axis_minor_length (29045,)\n",
      "[INFO][2023/01/10 10:37:47 AM] Writing properties/obj_type_1/eccentricity (29045,)\n",
      "[INFO][2023/01/10 10:37:47 AM] Writing properties/obj_type_1/area (29045,)\n",
      "[INFO][2023/01/10 10:37:47 AM] Writing properties/obj_type_1/orientation (29045,)\n",
      "[INFO][2023/01/10 10:37:47 AM] Writing properties/obj_type_1/mean_intensity-0 (29045,)\n",
      "[INFO][2023/01/10 10:37:47 AM] Writing properties/obj_type_1/mean_intensity-1 (29045,)\n",
      "[INFO][2023/01/10 10:37:47 AM] Writing properties/obj_type_1/mean_intensity-2 (29045,)\n",
      "[INFO][2023/01/10 10:37:47 AM] Writing properties/obj_type_1/mean_intensity-3 (29045,)\n",
      "[INFO][2023/01/10 10:37:47 AM] Writing tracks/obj_type_1\n",
      "[INFO][2023/01/10 10:37:47 AM] Writing LBEP/obj_type_1\n",
      "[INFO][2023/01/10 10:37:47 AM] Writing fates/obj_type_1\n",
      "[INFO][2023/01/10 10:37:47 AM] Closing HDF file: tracks.h5\n"
     ]
    }
   ],
   "source": [
    "with btrack.dataio.HDF5FileHandler(\"tracks.h5\", \"w\", obj_type=\"obj_type_1\") as hdf:\n",
    "    hdf.write_tracks(tracks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b460b422",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO][2023/01/10 10:42:26 AM] Opening HDF file: tracks.h5...\n",
      "[INFO][2023/01/10 10:42:26 AM] Loading tracks/obj_type_1\n",
      "[INFO][2023/01/10 10:42:26 AM] Loading LBEP/obj_type_1\n",
      "[INFO][2023/01/10 10:42:26 AM] Loading objects/obj_type_1 (29045, 5) (29045 filtered: None)\n",
      "[INFO][2023/01/10 10:42:26 AM] Closing HDF file: tracks.h5\n"
     ]
    }
   ],
   "source": [
    "with btrack.dataio.HDF5FileHandler('tracks.h5', \"r\", obj_type=\"obj_type_1\") as hdf:\n",
    "    tracks = hdf.tracks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e8cbbed4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(['from __future__ import annotations\\n',\n",
       "  '\\n',\n",
       "  'import csv\\n',\n",
       "  'import itertools\\n',\n",
       "  'import logging\\n',\n",
       "  'import os\\n',\n",
       "  'import re\\n',\n",
       "  'from functools import wraps\\n',\n",
       "  'from typing import TYPE_CHECKING, Any, Dict, List, Optional, Union\\n',\n",
       "  '\\n',\n",
       "  'import h5py\\n',\n",
       "  'import numpy as np\\n',\n",
       "  '\\n',\n",
       "  '# import core\\n',\n",
       "  'from . import btypes, constants, utils\\n',\n",
       "  '\\n',\n",
       "  'if TYPE_CHECKING:\\n',\n",
       "  '    from . import BayesianTracker\\n',\n",
       "  '\\n',\n",
       "  '# get the logger instance\\n',\n",
       "  'logger = logging.getLogger(__name__)\\n',\n",
       "  '\\n',\n",
       "  '\\n',\n",
       "  '# Choose a subset of classes/functions to document in public facing API\\n',\n",
       "  '__all__ = [\"import_CSV\"]\\n',\n",
       "  '\\n',\n",
       "  '\\n',\n",
       "  'def localizations_to_objects(\\n',\n",
       "  '    localizations: Union[\\n',\n",
       "  '        np.ndarray, List[btypes.PyTrackObject], Dict[str, Any]\\n',\n",
       "  '    ]\\n',\n",
       "  ') -> List[btypes.PyTrackObject]:\\n',\n",
       "  '    \"\"\"Take a numpy array or pandas dataframe and convert to PyTrackObjects.\\n',\n",
       "  '\\n',\n",
       "  '    Parameters\\n',\n",
       "  '    ----------\\n',\n",
       "  '    localizations : list[PyTrackObject], np.ndarray, pandas.DataFrame\\n',\n",
       "  '        A list or array of localizations.\\n',\n",
       "  '\\n',\n",
       "  '    Returns\\n',\n",
       "  '    -------\\n',\n",
       "  '    objects : list[PyTrackObject]\\n',\n",
       "  '        A list of PyTrackObject objects that represent the localizations.\\n',\n",
       "  '    \"\"\"\\n',\n",
       "  '\\n',\n",
       "  '    logger.info(f\"Objects are of type: {type(localizations)}\")\\n',\n",
       "  '\\n',\n",
       "  '    if isinstance(localizations, list):\\n',\n",
       "  '        if check_object_type(localizations):\\n',\n",
       "  '            # if these are already PyTrackObjects just silently return\\n',\n",
       "  '            return localizations\\n',\n",
       "  '\\n',\n",
       "  '    # do we have a numpy array or pandas dataframe?\\n',\n",
       "  '    if isinstance(localizations, np.ndarray):\\n',\n",
       "  '        return objects_from_array(localizations)\\n',\n",
       "  '    else:\\n',\n",
       "  '        try:\\n',\n",
       "  '            objects_dict = {\\n',\n",
       "  '                c: np.asarray(localizations[c]) for c in localizations\\n',\n",
       "  '            }\\n',\n",
       "  '        except ValueError:\\n',\n",
       "  '            logger.error(f\"Unknown localization type: {type(localizations)}\")\\n',\n",
       "  '            raise TypeError(\\n',\n",
       "  '                f\"Unknown localization type: {type(localizations)}\"\\n',\n",
       "  '            )\\n',\n",
       "  '\\n',\n",
       "  '    # how many objects are there\\n',\n",
       "  '    n_objects = objects_dict[\"t\"].shape[0]\\n',\n",
       "  '    objects_dict[\"ID\"] = np.arange(n_objects)\\n',\n",
       "  '\\n',\n",
       "  '    return objects_from_dict(objects_dict)\\n',\n",
       "  '\\n',\n",
       "  '\\n',\n",
       "  'def objects_from_dict(objects_dict: dict) -> List[btypes.PyTrackObject]:\\n',\n",
       "  '    \"\"\"Construct PyTrackObjects from a dictionary\"\"\"\\n',\n",
       "  '    # now that we have the object dictionary, convert this to objects\\n',\n",
       "  '    objects = []\\n',\n",
       "  '    n_objects = int(objects_dict[\"t\"].shape[0])\\n',\n",
       "  '\\n',\n",
       "  '    assert all([v.shape[0] == n_objects for k, v in objects_dict.items()])\\n',\n",
       "  '\\n',\n",
       "  '    for i in range(n_objects):\\n',\n",
       "  '        data = {k: v[i] for k, v in objects_dict.items()}\\n',\n",
       "  '        obj = btypes.PyTrackObject.from_dict(data)\\n',\n",
       "  '        objects.append(obj)\\n',\n",
       "  '    return objects\\n',\n",
       "  '\\n',\n",
       "  '\\n',\n",
       "  'def objects_from_array(\\n',\n",
       "  '    objects_arr: np.ndarray, default_keys=constants.DEFAULT_OBJECT_KEYS\\n',\n",
       "  ') -> List[btypes.PyTrackObject]:\\n',\n",
       "  '    \"\"\"Construct PyTrackObjects from a numpy array.\"\"\"\\n',\n",
       "  '    assert objects_arr.ndim == 2\\n',\n",
       "  '\\n',\n",
       "  '    n_features = objects_arr.shape[1]\\n',\n",
       "  '    assert n_features >= 3\\n',\n",
       "  '\\n',\n",
       "  '    n_objects = objects_arr.shape[0]\\n',\n",
       "  '\\n',\n",
       "  '    keys = default_keys[:n_features]\\n',\n",
       "  '    objects_dict = {keys[i]: objects_arr[:, i] for i in range(n_features)}\\n',\n",
       "  '    objects_dict[\"ID\"] = np.arange(n_objects)\\n',\n",
       "  '    return objects_from_dict(objects_dict)\\n',\n",
       "  '\\n',\n",
       "  '\\n',\n",
       "  'def import_JSON(filename: str):\\n',\n",
       "  '    \"\"\"Generic JSON importer for localisations from other software.\"\"\"\\n',\n",
       "  '    raise DeprecationWarning(\"`import_JSON has been deprecated`\")\\n',\n",
       "  '\\n',\n",
       "  '\\n',\n",
       "  'def import_CSV(filename: os.PathLike) -> List[btypes.PyTrackObject]:\\n',\n",
       "  '    \"\"\"Import localizations from a CSV file.\\n',\n",
       "  '\\n',\n",
       "  '    Parameters\\n',\n",
       "  '    ----------\\n',\n",
       "  '    filename : PathLike\\n',\n",
       "  '        The filename of the CSV to import\\n',\n",
       "  '\\n',\n",
       "  '    Returns\\n',\n",
       "  '    -------\\n',\n",
       "  '    objects : List[btypes.PyTrackObject]\\n',\n",
       "  '        A list of objects in the CSV file.\\n',\n",
       "  '\\n',\n",
       "  '    Notes\\n',\n",
       "  '    -----\\n',\n",
       "  '    CSV file should have one of the following format.\\n',\n",
       "  '\\n',\n",
       "  '    .. list-table:: CSV header format\\n',\n",
       "  '       :widths: 20 20 20 20 20\\n',\n",
       "  '       :header-rows: 1\\n',\n",
       "  '\\n',\n",
       "  '       * - t\\n',\n",
       "  '         - x\\n',\n",
       "  '         - y\\n',\n",
       "  '         - z\\n',\n",
       "  '         - label\\n',\n",
       "  '       * - required\\n',\n",
       "  '         - required\\n',\n",
       "  '         - required\\n',\n",
       "  '         - optional\\n',\n",
       "  '         - optional\\n',\n",
       "  '    \"\"\"\\n',\n",
       "  '\\n',\n",
       "  '    objects = []\\n',\n",
       "  '    with open(filename, \"r\") as csv_file:\\n',\n",
       "  '        csvreader = csv.DictReader(csv_file, delimiter=\",\", quotechar=\"|\")\\n',\n",
       "  '        for i, row in enumerate(csvreader):\\n',\n",
       "  '            data = {k: float(v) for k, v in row.items()}\\n',\n",
       "  '            data.update({\"ID\": i})\\n',\n",
       "  '            obj = btypes.PyTrackObject.from_dict(data)\\n',\n",
       "  '            objects.append(obj)\\n',\n",
       "  '    return objects\\n',\n",
       "  '\\n',\n",
       "  '\\n',\n",
       "  'def export_delegator(\\n',\n",
       "  '    filename: os.PathLike,\\n',\n",
       "  '    tracker: BayesianTracker,\\n',\n",
       "  '    obj_type: Optional[str] = None,\\n',\n",
       "  '    filter_by: Optional[str] = None,\\n',\n",
       "  ') -> None:\\n',\n",
       "  '    \"\"\"Export data from the tracker using the appropriate exporter.\\n',\n",
       "  '\\n',\n",
       "  '    Parameters\\n',\n",
       "  '    ----------\\n',\n",
       "  '    filename : str\\n',\n",
       "  '        The filename to export the data. The extension (e.g. .h5) is used\\n',\n",
       "  '        to select the correct export function.\\n',\n",
       "  '    tracker : BayesianTracker\\n',\n",
       "  '        An instance of the tracker.\\n',\n",
       "  '    obj_type : str, optional\\n',\n",
       "  '        The object type to export the data. Usually `obj_type_1`\\n',\n",
       "  '    filter_by : str, optional\\n',\n",
       "  '        A string that represents how the data has been filtered prior to\\n',\n",
       "  '        tracking, e.g. using the object property `area>100`\\n',\n",
       "  '\\n',\n",
       "  '    Notes\\n',\n",
       "  '    -----\\n',\n",
       "  '    This uses the appropriate exporter dependent on the given file extension.\\n',\n",
       "  '    \"\"\"\\n',\n",
       "  '    export_dir, export_fn = os.path.split(filename)\\n',\n",
       "  '    _, ext = os.path.splitext(filename)\\n',\n",
       "  '\\n',\n",
       "  '    if ext == \".csv\":\\n',\n",
       "  '        export_CSV(filename, tracker.tracks, obj_type=obj_type)\\n',\n",
       "  '    elif ext in (\".hdf\", \".hdf5\", \".h5\"):\\n',\n",
       "  '        _export_HDF(filename, tracker, obj_type=obj_type, filter_by=filter_by)\\n',\n",
       "  '    else:\\n',\n",
       "  '        logger.error(f\"Export file format {ext} not recognized.\")\\n',\n",
       "  '\\n',\n",
       "  '\\n',\n",
       "  'def check_track_type(tracks: list) -> bool:\\n',\n",
       "  '    return all(isinstance(t, btypes.Tracklet) for t in tracks)\\n',\n",
       "  '\\n',\n",
       "  '\\n',\n",
       "  'def check_object_type(objects: list) -> bool:\\n',\n",
       "  '    return all(isinstance(o, btypes.PyTrackObject) for o in objects)\\n',\n",
       "  '\\n',\n",
       "  '\\n',\n",
       "  'def export_CSV(\\n',\n",
       "  '    filename: os.PathLike,\\n',\n",
       "  '    tracks: list,\\n',\n",
       "  '    properties: list = constants.DEFAULT_EXPORT_PROPERTIES,\\n',\n",
       "  '    obj_type: Optional[str] = None,\\n',\n",
       "  '):\\n',\n",
       "  '    \"\"\"Export the track data as a simple CSV file.\\n',\n",
       "  '\\n',\n",
       "  '    Parameters\\n',\n",
       "  '    ----------\\n',\n",
       "  '    filename : str\\n',\n",
       "  '        The filename of the file to be exported.\\n',\n",
       "  '    tracks : list[Tracklet]\\n',\n",
       "  '        A list of Tracklet objects to be exported.\\n',\n",
       "  '    properties : list, default = constants.DEFAULT_EXPORT_PROPERTIES\\n',\n",
       "  '        A list of tracklet properties to be exported.\\n',\n",
       "  '    obj_type : str, optional\\n',\n",
       "  '        A string describing the object type, e.g. `obj_type_1`.\\n',\n",
       "  '\\n',\n",
       "  '    \"\"\"\\n',\n",
       "  '\\n',\n",
       "  '    if not tracks:\\n',\n",
       "  '        logger.error(f\"No tracks found when exporting to: {filename}\")\\n',\n",
       "  '        return\\n',\n",
       "  '\\n',\n",
       "  '    if not check_track_type(tracks):\\n',\n",
       "  '        logger.error(\"Tracks of incorrect type\")\\n',\n",
       "  '\\n',\n",
       "  '    logger.info(f\"Writing out CSV files to: {filename}\")\\n',\n",
       "  '    export_track = np.vstack([t.to_array(properties) for t in tracks])\\n',\n",
       "  '\\n',\n",
       "  '    with open(filename, \"w\", newline=\"\") as csv_file:\\n',\n",
       "  '        csvwriter = csv.writer(csv_file, delimiter=\" \")\\n',\n",
       "  '        csvwriter.writerow(properties)\\n',\n",
       "  '        for i in range(export_track.shape[0]):\\n',\n",
       "  '            csvwriter.writerow(export_track[i, :].tolist())\\n',\n",
       "  '\\n',\n",
       "  '\\n',\n",
       "  'def export_LBEP(filename: os.PathLike, tracks: list):\\n',\n",
       "  '    \"\"\"Export the LBEP table as a text file.\"\"\"\\n',\n",
       "  '    if not tracks:\\n',\n",
       "  '        logger.error(f\"No tracks found when exporting to: {filename}\")\\n',\n",
       "  '        return\\n',\n",
       "  '\\n',\n",
       "  '    if not check_track_type(tracks):\\n',\n",
       "  '        logger.error(\"Tracks of incorrect type\")\\n',\n",
       "  '\\n',\n",
       "  '    tracks.sort(key=lambda t: t.ID)\\n',\n",
       "  '\\n',\n",
       "  '    with open(filename, \"w\") as lbep_file:\\n',\n",
       "  '        logger.info(f\"Writing LBEP file: {filename}...\")\\n',\n",
       "  '        for track in tracks:\\n',\n",
       "  '            lbep = f\"{track.ID} {track.t[0]} {track.t[-1]} {track.parent}\"\\n',\n",
       "  '            lbep_file.write(f\"{lbep}\\\\n\")\\n',\n",
       "  '\\n',\n",
       "  '\\n',\n",
       "  'def _export_HDF(\\n',\n",
       "  '    filename: os.PathLike, tracker, obj_type=None, filter_by: str = None\\n',\n",
       "  '):\\n',\n",
       "  '    \"\"\"Export to HDF.\"\"\"\\n',\n",
       "  '\\n',\n",
       "  '    filename_noext, ext = os.path.splitext(filename)\\n',\n",
       "  '    if not ext == \".h5\":\\n',\n",
       "  '        filename = filename_noext + \".h5\"\\n',\n",
       "  '        logger.warning(f\"Changing HDF filename to {filename}\")\\n',\n",
       "  '\\n',\n",
       "  '    with HDF5FileHandler(filename, read_write=\"a\", obj_type=obj_type) as hdf:\\n',\n",
       "  '        # if there are no objects, write them out\\n',\n",
       "  '        if f\"objects/{obj_type}\" not in hdf._hdf:\\n',\n",
       "  '            hdf.write_objects(tracker)\\n',\n",
       "  '        # write the tracks\\n',\n",
       "  '        hdf.write_tracks(tracker, f_expr=filter_by)\\n',\n",
       "  '\\n',\n",
       "  '\\n',\n",
       "  'def h5check_property_exists(property):\\n',\n",
       "  '    \"\"\"Wrapper for hdf handler to make sure a property exists.\"\"\"\\n',\n",
       "  '\\n',\n",
       "  '    def func(fn):\\n',\n",
       "  '        @wraps(fn)\\n',\n",
       "  '        def wrapped_handler_property(*args, **kwargs):\\n',\n",
       "  '            self = args[0]\\n',\n",
       "  '            assert isinstance(self, HDF5FileHandler)\\n',\n",
       "  '            if property not in self._hdf:\\n',\n",
       "  '                logger.error(\\n',\n",
       "  '                    f\"{property.capitalize()} not found in {self.filename}\"\\n',\n",
       "  '                )\\n',\n",
       "  '                return None\\n',\n",
       "  '            return fn(*args, **kwargs)\\n',\n",
       "  '\\n',\n",
       "  '        return wrapped_handler_property\\n',\n",
       "  '\\n',\n",
       "  '    return func\\n',\n",
       "  '\\n',\n",
       "  '\\n',\n",
       "  'class HDF5FileHandler:\\n',\n",
       "  '    \"\"\"Generic HDF5 file hander for reading and writing datasets. This is\\n',\n",
       "  '    inter-operable between segmentation, tracking and analysis code.\\n',\n",
       "  '\\n',\n",
       "  '    Parameters\\n',\n",
       "  '    ----------\\n',\n",
       "  '    filename : str\\n',\n",
       "  '        The filename of the hdf5 file to be used.\\n',\n",
       "  '    read_write : str\\n',\n",
       "  '        A read/write mode for the file, e.g. `w`, `r`, `a` etc.\\n',\n",
       "  '    obj_type : str\\n',\n",
       "  '        The name of the object type. Defaults to `obj_type_1`. The object type\\n',\n",
       "  '        name must start with `obj_type_`\\n',\n",
       "  '\\n',\n",
       "  '    Attributes\\n',\n",
       "  '    ----------\\n',\n",
       "  '    segmentation : np.ndarray\\n',\n",
       "  '        A numpy array representing the segmentation data. TZYX\\n',\n",
       "  '    objects : list [PyTrackObject]\\n',\n",
       "  '        A list of PyTrackObjects localised from the segmentation data.\\n',\n",
       "  '    filtered_objects  : np.ndarray\\n',\n",
       "  '        Similar to objects, but filtered by property.\\n',\n",
       "  '    tracks : list [Tracklet]\\n',\n",
       "  '        A list of Tracklet objects.\\n',\n",
       "  '    lbep : np.ndarray\\n',\n",
       "  '        The LBEP table representing the track graph.\\n',\n",
       "  '\\n',\n",
       "  '    Notes\\n',\n",
       "  '    -----\\n',\n",
       "  '    ```\\n',\n",
       "  '    Basic format of the HDF file is:\\n',\n",
       "  '        segmentation/\\n',\n",
       "  '            images          - (J x (d) x h x w) uint16 segmentation\\n',\n",
       "  '        objects/\\n',\n",
       "  '            obj_type_1/\\n',\n",
       "  '                coords      - (I x 5) [t, x, y, z, object_type]\\n',\n",
       "  '                labels      - (I x D) [label, (softmax scores ...)]\\n',\n",
       "  '                map         - (J x 2) [start_index, end_index] -> coords array\\n',\n",
       "  '                properties/\\n',\n",
       "  '                    area  - (I x 1) first named property (e.g. `area`)\\n',\n",
       "  '                    ...\\n',\n",
       "  '            ...\\n',\n",
       "  '        tracks/\\n',\n",
       "  '            obj_type_1/\\n',\n",
       "  '                tracks      - (I x 1) [index into coords]\\n',\n",
       "  '                dummies     - similar to coords, but for dummy objects\\n',\n",
       "  '                map         - (K x 2) [start_index, end_index] -> tracks array\\n',\n",
       "  '                LBEPRG      - (K x 6) [L, B, E, P, R, G]\\n',\n",
       "  '                fates       - (K x n) [fate_from_tracker, ...future_expansion]\\n',\n",
       "  '            ...\\n',\n",
       "  '\\n',\n",
       "  '    Where:\\n',\n",
       "  '        I - number of objects\\n',\n",
       "  '        J - number of frames\\n',\n",
       "  '        K - number of tracks\\n',\n",
       "  '    ```\\n',\n",
       "  '\\n',\n",
       "  '    LBEPR is a modification of the LBEP format to also include the root node\\n',\n",
       "  '    of the tree.\\n',\n",
       "  '\\n',\n",
       "  '\\n',\n",
       "  '    Examples\\n',\n",
       "  '    --------\\n',\n",
       "  '    Read objects from a file:\\n',\n",
       "  \"    >>> with HDF5FileHandler('file.h5', 'r') as handler:\\n\",\n",
       "  '    >>>    objects = handler.objects\\n',\n",
       "  '\\n',\n",
       "  '    Use filtering by property for object retrieval:\\n',\n",
       "  \"    >>> obj = handler.filtered_objects('flag==1')\\n\",\n",
       "  \"    >>> obj = handler.filtered_objects('area>100')\\n\",\n",
       "  '\\n',\n",
       "  '    Write tracks directly to a file:\\n',\n",
       "  '    >>> handler.write_tracks(tracks)\\n',\n",
       "  '    \"\"\"\\n',\n",
       "  '\\n',\n",
       "  '    def __init__(\\n',\n",
       "  '        self,\\n',\n",
       "  '        filename: os.PathLike,\\n',\n",
       "  '        read_write: str = \"r\",\\n',\n",
       "  '        *,\\n',\n",
       "  '        obj_type: str = \"obj_type_1\",\\n',\n",
       "  '    ):\\n',\n",
       "  '\\n',\n",
       "  '        self._f_expr = None  # DO NOT USE\\n',\n",
       "  '        self.object_type = obj_type\\n',\n",
       "  '\\n',\n",
       "  '        self.filename = filename\\n',\n",
       "  '        logger.info(f\"Opening HDF file: {self.filename}...\")\\n',\n",
       "  '        self._hdf = h5py.File(filename, read_write)\\n',\n",
       "  '        self._states = list(constants.States)\\n',\n",
       "  '\\n',\n",
       "  '    @property\\n',\n",
       "  '    def object_types(self) -> List[str]:\\n',\n",
       "  '        return list(self._hdf[\"objects\"].keys())\\n',\n",
       "  '\\n',\n",
       "  '    def __enter__(self):\\n',\n",
       "  '        return self\\n',\n",
       "  '\\n',\n",
       "  '    def __exit__(self, exc_type, exc_value, traceback):\\n',\n",
       "  '        self.close()\\n',\n",
       "  '\\n',\n",
       "  '    def close(self):\\n',\n",
       "  '        if not self._hdf:\\n',\n",
       "  '            return\\n',\n",
       "  '        logger.info(f\"Closing HDF file: {self.filename}\")\\n',\n",
       "  '        self._hdf.close()\\n',\n",
       "  '\\n',\n",
       "  '    @property\\n',\n",
       "  '    def object_type(self) -> str:\\n',\n",
       "  '        return self._object_type\\n',\n",
       "  '\\n',\n",
       "  '    @object_type.setter\\n',\n",
       "  '    def object_type(self, obj_type: str) -> None:\\n',\n",
       "  '        if not obj_type.startswith(\"obj_type_\"):\\n',\n",
       "  '            raise ValueError(\"Object type must start with ``obj_type_``\")\\n',\n",
       "  '        self._object_type = obj_type\\n',\n",
       "  '\\n',\n",
       "  '    @property  # type: ignore\\n',\n",
       "  '    @h5check_property_exists(\"segmentation\")\\n',\n",
       "  '    def segmentation(self) -> np.ndarray:\\n',\n",
       "  '        segmentation = self._hdf[\"segmentation\"][\"images\"][:].astype(np.uint16)\\n',\n",
       "  '        logger.info(f\"Loading segmentation {segmentation.shape}\")\\n',\n",
       "  '        return segmentation\\n',\n",
       "  '\\n',\n",
       "  '    def write_segmentation(self, segmentation: np.ndarray) -> None:\\n',\n",
       "  '        \"\"\"Write out the segmentation to an HDF file.\\n',\n",
       "  '\\n',\n",
       "  '        Parameters\\n',\n",
       "  '        ----------\\n',\n",
       "  '        segmentation : np.ndarray\\n',\n",
       "  '            A numpy array representing the segmentation data. T(Z)YX, uint16\\n',\n",
       "  '        \"\"\"\\n',\n",
       "  '        # write the segmentation out\\n',\n",
       "  '        grp = self._hdf.create_group(\"segmentation\")\\n',\n",
       "  '        grp.create_dataset(\\n',\n",
       "  '            \"images\",\\n',\n",
       "  '            data=segmentation,\\n',\n",
       "  '            dtype=\"uint16\",\\n',\n",
       "  '            compression=\"gzip\",\\n',\n",
       "  '            compression_opts=7,\\n',\n",
       "  '        )\\n',\n",
       "  '\\n',\n",
       "  '    @property\\n',\n",
       "  '    def objects(self) -> List[btypes.PyTrackObject]:\\n',\n",
       "  '        \"\"\"Return the objects in the file.\"\"\"\\n',\n",
       "  '        return self.filtered_objects()\\n',\n",
       "  '\\n',\n",
       "  '    @h5check_property_exists(\"objects\")\\n',\n",
       "  '    def filtered_objects(\\n',\n",
       "  '        self, f_expr: Optional[str] = None\\n',\n",
       "  '    ) -> List[btypes.PyTrackObject]:\\n',\n",
       "  '        \"\"\"A filtered list of objects based on metadata. f_expr should be of the\\n',\n",
       "  '        format `flag==1`.\"\"\"\\n',\n",
       "  '\\n',\n",
       "  '        if self.object_type not in self.object_types:\\n',\n",
       "  '            raise ValueError(f\"Object type {self.object_type} not recognized\")\\n',\n",
       "  '\\n',\n",
       "  '        grp = self._hdf[\"objects\"][self.object_type]\\n',\n",
       "  '\\n',\n",
       "  '        # read the whole dataset into memory\\n',\n",
       "  '        txyz = grp[\"coords\"][:]\\n',\n",
       "  '        if \"labels\" not in grp:\\n',\n",
       "  '            logger.warning(\"Labels missing from objects in HDF file\")\\n',\n",
       "  '            labels = np.zeros((txyz.shape[0], 6))\\n',\n",
       "  '        else:\\n',\n",
       "  '            labels = self._hdf[\"objects\"][self.object_type][\"labels\"][:]\\n',\n",
       "  '\\n',\n",
       "  '        # get properties if we have them (note, this assumes that the same\\n',\n",
       "  '        # properties exist for each object)\\n',\n",
       "  '        properties = {}\\n',\n",
       "  '        if \"properties\" in grp:\\n',\n",
       "  '            properties = {\\n',\n",
       "  '                k: grp[\"properties\"][k][:] for k in grp[\"properties\"]\\n',\n",
       "  '            }\\n',\n",
       "  '            assert all([len(p) == len(txyz) for p in properties.values()])\\n',\n",
       "  '\\n',\n",
       "  \"        # note that this doesn't do much error checking at the moment\\n\",\n",
       "  '        # TODO(arl): this should now reference the `properties`\\n',\n",
       "  '        if f_expr is not None:\\n',\n",
       "  '            assert isinstance(f_expr, str)\\n',\n",
       "  '            pattern = r\"(?P<name>\\\\w+)(?P<op>[\\\\>\\\\<\\\\=]+)(?P<cmp>[0-9]+)\"\\n',\n",
       "  '            m = re.match(pattern, f_expr)\\n',\n",
       "  '\\n',\n",
       "  '            if m is None:\\n',\n",
       "  '                raise ValueError(f\"Cannot filter objects by {f_expr}\")\\n',\n",
       "  '\\n',\n",
       "  '            f_eval = f\"x{m[\\'op\\']}{m[\\'cmp\\']}\"  # e.g. x > 10\\n',\n",
       "  '\\n',\n",
       "  '            # old files have these stored differently\\n',\n",
       "  '            if \"properties\" in grp.keys():\\n',\n",
       "  '                property_group = grp[\"properties\"]\\n',\n",
       "  '            else:\\n',\n",
       "  '                property_group = grp\\n',\n",
       "  '\\n',\n",
       "  '            if m[\"name\"] in property_group.keys():\\n',\n",
       "  '                # logger.info(f\"Property {m[\\'name\\']} found in {property_group}.\")\\n',\n",
       "  '                data = property_group[m[\"name\"]][:]\\n',\n",
       "  '                filtered_idx = [i for i, x in enumerate(data) if eval(f_eval)]\\n',\n",
       "  '            else:\\n',\n",
       "  '                raise ValueError(f\"Cannot filter objects by {f_expr}\")\\n',\n",
       "  '\\n',\n",
       "  '        else:\\n',\n",
       "  '            filtered_idx = range(txyz.shape[0])  # default filtering uses all\\n',\n",
       "  '\\n',\n",
       "  '        # sanity check that coordinates matches labels\\n',\n",
       "  '        assert txyz.shape[0] == labels.shape[0]\\n',\n",
       "  '        logger.info(\\n',\n",
       "  '            f\"Loading objects/{self.object_type} {txyz.shape} \"\\n',\n",
       "  '            f\"({len(filtered_idx)} filtered: {f_expr})\"\\n',\n",
       "  '        )\\n',\n",
       "  '\\n',\n",
       "  '        txyz_filtered = txyz[filtered_idx, :]\\n',\n",
       "  '        labels_filtered = labels[filtered_idx, :]\\n',\n",
       "  '\\n',\n",
       "  '        objects_dict = {\\n',\n",
       "  '            \"t\": txyz_filtered[:, 0],\\n',\n",
       "  '            \"x\": txyz_filtered[:, 1],\\n',\n",
       "  '            \"y\": txyz_filtered[:, 2],\\n',\n",
       "  '            \"z\": txyz_filtered[:, 3],\\n',\n",
       "  '            \"label\": labels_filtered[:, 0],\\n',\n",
       "  '            \"ID\": np.asarray(filtered_idx),\\n',\n",
       "  '        }\\n',\n",
       "  '\\n',\n",
       "  '        # add the filtered properties\\n',\n",
       "  '        for key, props in properties.items():\\n',\n",
       "  '            objects_dict.update({key: props[filtered_idx]})\\n',\n",
       "  '\\n',\n",
       "  '        return objects_from_dict(objects_dict)\\n',\n",
       "  '\\n',\n",
       "  '    def write_objects(\\n',\n",
       "  '        self, data: Union[List[btypes.PyTrackObject, BayesianTracker]]\\n',\n",
       "  '    ) -> None:\\n',\n",
       "  '        \"\"\"Write objects to HDF file.\\n',\n",
       "  '\\n',\n",
       "  '        Parameters\\n',\n",
       "  '        ----------\\n',\n",
       "  '        data : list or BayesianTracker instance\\n',\n",
       "  '            Either a list of PyTrackObject to be written, or an instance of\\n',\n",
       "  '            BayesianTracker with a .objects property.\\n',\n",
       "  '        \"\"\"\\n',\n",
       "  '        # TODO(arl): make sure that the objects are ordered in time\\n',\n",
       "  '\\n',\n",
       "  '        if isinstance(data, list):\\n',\n",
       "  '            objects = data\\n',\n",
       "  '        elif hasattr(data, \"objects\"):\\n',\n",
       "  '            objects = data.objects\\n',\n",
       "  '        else:\\n',\n",
       "  '            raise TypeError(\"Object type not recognized.\")\\n',\n",
       "  '\\n',\n",
       "  '        # make sure that the data to be written are all of type PyTrackObject\\n',\n",
       "  '        if not check_object_type(objects):\\n',\n",
       "  '            raise TypeError(\"Object type not recognized.\")\\n',\n",
       "  '\\n',\n",
       "  '        if \"objects\" not in self._hdf:\\n',\n",
       "  '            self._hdf.create_group(\"objects\")\\n',\n",
       "  '        grp = self._hdf[\"objects\"].create_group(self.object_type)\\n',\n",
       "  '        props = {k: [] for k in objects[0].properties.keys()}\\n',\n",
       "  '\\n',\n",
       "  '        n_objects = len(objects)\\n',\n",
       "  '        n_frames = np.max([o.t for o in objects]) + 1\\n',\n",
       "  '\\n',\n",
       "  '        txyz = np.zeros((n_objects, 5), dtype=np.float32)\\n',\n",
       "  '        labels = np.zeros((n_objects, 1), dtype=np.uint8)\\n',\n",
       "  '        fmap = np.zeros((n_frames, 2), dtype=np.uint32)\\n',\n",
       "  '\\n',\n",
       "  '        # convert the btrack objects into a numpy array\\n',\n",
       "  '        for i, obj in enumerate(objects):\\n',\n",
       "  '            txyz[i, :] = [obj.t, obj.x, obj.y, obj.z, 0]\\n',\n",
       "  '            labels[i, :] = obj.label\\n',\n",
       "  '            t = int(obj.t)\\n',\n",
       "  '            fmap[t, 1] = np.max([fmap[t, 1], i])\\n',\n",
       "  '\\n',\n",
       "  '            # add in any properties\\n',\n",
       "  '            for key in props.keys():\\n',\n",
       "  '                props[key].append(obj.properties[key])\\n',\n",
       "  '\\n',\n",
       "  '        fmap[1:, 0] = fmap[:-1, 1]\\n',\n",
       "  '\\n',\n",
       "  '        logger.info(f\"Writing objects/{self.object_type}\")\\n',\n",
       "  '        grp.create_dataset(\"coords\", data=txyz, dtype=\"float32\")\\n',\n",
       "  '        grp.create_dataset(\"map\", data=fmap, dtype=\"uint32\")\\n',\n",
       "  '\\n',\n",
       "  '        logger.info(f\"Writing labels/{self.object_type}\")\\n',\n",
       "  '        grp.create_dataset(\"labels\", data=labels, dtype=\"float32\")\\n',\n",
       "  '\\n',\n",
       "  '        # finally, write any properties\\n',\n",
       "  '        self.write_properties(props)\\n',\n",
       "  '\\n',\n",
       "  '    @h5check_property_exists(\"objects\")\\n',\n",
       "  '    def write_properties(\\n',\n",
       "  '        self, data: Dict[str, Any], *, allow_overwrite: bool = False\\n',\n",
       "  '    ) -> None:\\n',\n",
       "  '        \"\"\"Write object properties to HDF file.\\n',\n",
       "  '\\n',\n",
       "  '        Parameters\\n',\n",
       "  '        ----------\\n',\n",
       "  '        data : dict {key: (N, D)}\\n',\n",
       "  '            A dictionary of key-value pairs of properties to be written. The\\n',\n",
       "  '            values should be an array equal in length to the number of objects\\n',\n",
       "  '            and with D dimensions.\\n',\n",
       "  '        allow_overwrite : bool\\n',\n",
       "  '            Allow to delete the existing property keys from the HDF5 file and\\n',\n",
       "  '            overwrite with new values from the data dict. Defaults to False.\\n',\n",
       "  '        \"\"\"\\n',\n",
       "  '\\n',\n",
       "  '        if not isinstance(data, dict):\\n',\n",
       "  '            raise TypeError(\"Properties must be a dictionary.\")\\n',\n",
       "  '\\n',\n",
       "  '        grp = self._hdf[f\"objects/{self.object_type}\"]\\n',\n",
       "  '\\n',\n",
       "  '        if \"properties\" not in grp.keys():\\n',\n",
       "  '            props_grp = grp.create_group(\"properties\")\\n',\n",
       "  '        else:\\n',\n",
       "  '            props_grp = self._hdf[f\"objects/{self.object_type}/properties\"]\\n',\n",
       "  '\\n',\n",
       "  '        n_objects = len(self.objects)\\n',\n",
       "  '\\n',\n",
       "  '        for key, values in data.items():\\n',\n",
       "  '            # Manage the property data:\\n',\n",
       "  '            if not values:\\n',\n",
       "  '                logger.warning(f\"Property \\'{key}\\' is empty.\")\\n',\n",
       "  '                continue\\n',\n",
       "  '            values = np.array(values)\\n',\n",
       "  '            assert values.shape[0] == n_objects\\n',\n",
       "  '\\n',\n",
       "  '            # Check if the property is already in the props_grp:\\n',\n",
       "  '            if key in props_grp:\\n',\n",
       "  '                if allow_overwrite is False:\\n',\n",
       "  '                    logger.info(\\n',\n",
       "  '                        f\"Property \\'{key}\\' already written in the file\"\\n',\n",
       "  '                    )\\n',\n",
       "  '                    raise KeyError(\\n',\n",
       "  '                        f\"Property \\'{key}\\' already in file -> switch on \"\\n',\n",
       "  '                        \"\\'overwrite\\' param to replace existing property \"\\n',\n",
       "  '                    )\\n',\n",
       "  '                else:\\n',\n",
       "  '                    del self._hdf[f\"objects/{self.object_type}/properties\"][\\n',\n",
       "  '                        key\\n',\n",
       "  '                    ]\\n',\n",
       "  '                    logger.info(\\n',\n",
       "  '                        f\"Property \\'{key}\\' erased to be overwritten...\"\\n',\n",
       "  '                    )\\n',\n",
       "  '\\n',\n",
       "  '            # Now that you handled overwriting, write the values:\\n',\n",
       "  '            logger.info(\\n',\n",
       "  '                f\"Writing properties/{self.object_type}/{key} {values.shape}\"\\n',\n",
       "  '            )\\n',\n",
       "  '            props_grp.create_dataset(key, data=data[key], dtype=\"float32\")\\n',\n",
       "  '\\n',\n",
       "  '    @property  # type: ignore\\n',\n",
       "  '    @h5check_property_exists(\"tracks\")\\n',\n",
       "  '    def tracks(self) -> List[btypes.Tracklet]:\\n',\n",
       "  '        \"\"\"Return the tracks in the file.\"\"\"\\n',\n",
       "  '\\n',\n",
       "  '        logger.info(f\"Loading tracks/{self.object_type}\")\\n',\n",
       "  '        track_map = self._hdf[\"tracks\"][self.object_type][\"map\"][:]\\n',\n",
       "  '        track_refs = self._hdf[\"tracks\"][self.object_type][\"tracks\"][:]\\n',\n",
       "  '        lbep = self.lbep\\n',\n",
       "  '        fates = self._hdf[\"tracks\"][self.object_type][\"fates\"][:]\\n',\n",
       "  '\\n',\n",
       "  '        # if there are dummies, make new dummy objects\\n',\n",
       "  '        if \"dummies\" in self._hdf[\"tracks\"][self.object_type]:\\n',\n",
       "  '            dummies = self._hdf[\"tracks\"][self.object_type][\"dummies\"][:]\\n',\n",
       "  '            dummy_obj = objects_from_array(dummies[:, :4])\\n',\n",
       "  '            for d in dummy_obj:\\n',\n",
       "  '                d.ID = -(d.ID + 1)  # restore the -ve ID\\n',\n",
       "  '                d.dummy = True  # set the dummy flag to true\\n',\n",
       "  '\\n',\n",
       "  '        # TODO(arl): this needs to be stored in the HDF folder\\n',\n",
       "  '        if (\\n',\n",
       "  '            \"f_expr\" in self._hdf[\"tracks\"][self.object_type].attrs\\n',\n",
       "  '            and self._f_expr is None\\n',\n",
       "  '        ):\\n',\n",
       "  '            f_expr = self._hdf[\"tracks\"][self.object_type].attrs[\"f_expr\"]\\n',\n",
       "  '        elif self._f_expr is not None:\\n',\n",
       "  '            f_expr = self._f_expr\\n',\n",
       "  '        else:\\n',\n",
       "  '            f_expr = None\\n',\n",
       "  '\\n',\n",
       "  '        obj = self.filtered_objects(f_expr=f_expr)\\n',\n",
       "  '\\n',\n",
       "  '        def _get_txyz(_ref: int) -> int:\\n',\n",
       "  '            if _ref >= 0:\\n',\n",
       "  '                return obj[_ref]\\n',\n",
       "  '            return dummy_obj[abs(_ref) - 1]  # references are -ve for dummies\\n',\n",
       "  '\\n',\n",
       "  '        tracks = []\\n',\n",
       "  '        for i in range(track_map.shape[0]):\\n',\n",
       "  '            idx = slice(*track_map[i, :].tolist())\\n',\n",
       "  '            refs = track_refs[idx]\\n',\n",
       "  '            track = btypes.Tracklet(lbep[i, 0], list(map(_get_txyz, refs)))\\n',\n",
       "  '            track.parent = lbep[i, 3]  # set the parent and root of tree\\n',\n",
       "  '            track.root = lbep[i, 4]\\n',\n",
       "  '            if lbep.shape[1] > 5:\\n',\n",
       "  '                track.generation = lbep[i, 5]\\n',\n",
       "  '            track.fate = constants.Fates(fates[i])  # restore the track fate\\n',\n",
       "  '            tracks.append(track)\\n',\n",
       "  '\\n',\n",
       "  '        # once we have all of the tracks, populate the children\\n',\n",
       "  '        to_update = {}\\n',\n",
       "  '        for track in tracks:\\n',\n",
       "  '            if not track.is_root:\\n',\n",
       "  '                parents = filter(lambda t: t.ID == track.parent, tracks)\\n',\n",
       "  '                for parent in parents:\\n',\n",
       "  '                    if parent not in to_update:\\n',\n",
       "  '                        to_update[parent] = []\\n',\n",
       "  '                    to_update[parent].append(track.ID)\\n',\n",
       "  '\\n',\n",
       "  '        # sanity check, can be removed at a later date\\n',\n",
       "  '        assert all([len(children) <= 2 for children in to_update.values()])\\n',\n",
       "  '\\n',\n",
       "  '        # add the children to the parent\\n',\n",
       "  '        for track, children in to_update.items():\\n',\n",
       "  '            track.children = children\\n',\n",
       "  '\\n',\n",
       "  '        return tracks\\n',\n",
       "  '\\n',\n",
       "  '    def write_tracks(\\n',\n",
       "  '        self,\\n',\n",
       "  '        data: Union[List[btypes.Tracklet], BayesianTracker],\\n',\n",
       "  '        *,\\n',\n",
       "  '        f_expr: Optional[str] = None,\\n',\n",
       "  '    ) -> None:\\n',\n",
       "  '        \"\"\"Write tracks to HDF file.\\n',\n",
       "  '\\n',\n",
       "  '        Parameters\\n',\n",
       "  '        ----------\\n',\n",
       "  '        data : list of Tracklets or an instance of BayesianTracker\\n',\n",
       "  '            A list of tracklets or an instance of BayesianTracker.\\n',\n",
       "  '        f_expr : str\\n',\n",
       "  '            An expression which represents how the objects have been filtered\\n',\n",
       "  '            prior to tracking, e.g. `area>100.0`\\n',\n",
       "  '        \"\"\"\\n',\n",
       "  '\\n',\n",
       "  '        if isinstance(data, list):\\n',\n",
       "  '            if not check_track_type(data):\\n',\n",
       "  '                raise ValueError(f\"Data of type {type(data)} not supported.\")\\n',\n",
       "  '\\n',\n",
       "  '            all_objects = itertools.chain.from_iterable(\\n',\n",
       "  '                [trk._data for trk in data]\\n',\n",
       "  '            )\\n',\n",
       "  '\\n',\n",
       "  '            objects = [obj for obj in all_objects if not obj.dummy]\\n',\n",
       "  '            dummies = [obj for obj in all_objects if obj.dummy]\\n',\n",
       "  '\\n',\n",
       "  '            # renumber the object ID so that they can be stored in a contiguous\\n',\n",
       "  '            # array and indexed by row - this may not be necessary for most\\n',\n",
       "  '            # datasets, but is here just in case\\n',\n",
       "  '            for idx, obj in enumerate(objects):\\n',\n",
       "  '                obj.ID = idx\\n',\n",
       "  '\\n',\n",
       "  '            for idx, dummy in enumerate(dummies):\\n',\n",
       "  '                dummy.ID = -(idx + 1)\\n',\n",
       "  '\\n',\n",
       "  '            refs = [trk.refs for trk in data]\\n',\n",
       "  '            lbep_table = utils._lbep_table(data)\\n',\n",
       "  '            fate_table = np.stack([t.fate.value for t in data], axis=0)\\n',\n",
       "  '\\n',\n",
       "  '            if \"objects\" not in self._hdf:\\n',\n",
       "  '                self.write_objects(objects)\\n',\n",
       "  '\\n',\n",
       "  '        elif hasattr(data, \"tracks\"):\\n',\n",
       "  '            refs = data.refs\\n',\n",
       "  '            dummies = data.dummies\\n',\n",
       "  '            lbep_table = data.LBEP\\n',\n",
       "  '            fate_table = np.stack([t.fate.value for t in data.tracks], axis=0)\\n',\n",
       "  '        else:\\n',\n",
       "  '            raise ValueError(f\"Data of type {type(data)} not supported.\")\\n',\n",
       "  '\\n',\n",
       "  '        if not refs:\\n',\n",
       "  '            logger.error(f\"No tracks found when exporting to: {self.filename}\")\\n',\n",
       "  '            return\\n',\n",
       "  '\\n',\n",
       "  '        # sanity check\\n',\n",
       "  '        assert lbep_table.shape[0] == len(refs)\\n',\n",
       "  '\\n',\n",
       "  '        logger.info(f\"Writing tracks/{self.object_type}\")\\n',\n",
       "  '        hdf_tracks = np.concatenate(refs, axis=0)\\n',\n",
       "  '\\n',\n",
       "  '        hdf_frame_map = np.zeros((len(refs), 2), dtype=np.int32)\\n',\n",
       "  '        for i, track in enumerate(refs):\\n',\n",
       "  '            if i > 0:\\n',\n",
       "  '                offset = hdf_frame_map[i - 1, 1]\\n',\n",
       "  '            else:\\n',\n",
       "  '                offset = 0\\n',\n",
       "  '            hdf_frame_map[i, :] = np.array([0, len(track)]) + offset\\n',\n",
       "  '\\n',\n",
       "  '        if \"tracks\" not in self._hdf:\\n',\n",
       "  '            self._hdf.create_group(\"tracks\")\\n',\n",
       "  '\\n',\n",
       "  '        if self.object_type in self._hdf[\"tracks\"]:\\n',\n",
       "  '            logger.warning(f\"Removing tracks/{self.object_type}.\")\\n',\n",
       "  '            del self._hdf[\"tracks\"][self.object_type]\\n',\n",
       "  '\\n',\n",
       "  '        grp = self._hdf[\"tracks\"].create_group(self.object_type)\\n',\n",
       "  '        grp.create_dataset(\"tracks\", data=hdf_tracks, dtype=\"int32\")\\n',\n",
       "  '        grp.create_dataset(\"map\", data=hdf_frame_map, dtype=\"uint32\")\\n',\n",
       "  '\\n',\n",
       "  '        # if we have used the f_expr we can save it as an attribute here\\n',\n",
       "  '        if f_expr is not None and isinstance(f_expr, str):\\n',\n",
       "  '            grp.attrs[\"f_expr\"] = f_expr\\n',\n",
       "  '\\n',\n",
       "  '        # also save the version number as an attribute\\n',\n",
       "  '        grp.attrs[\"version\"] = constants.get_version()\\n',\n",
       "  '\\n',\n",
       "  '        # write out dummies\\n',\n",
       "  '        if dummies:\\n',\n",
       "  '            logger.info(f\"Writing dummies/{self.object_type}\")\\n',\n",
       "  '            o = self.object_types.index(self.object_type) + 1\\n',\n",
       "  '            txyz = np.stack([[d.t, d.x, d.y, d.z, o] for d in dummies], axis=0)\\n',\n",
       "  '            grp.create_dataset(\"dummies\", data=txyz, dtype=\"float32\")\\n',\n",
       "  '\\n',\n",
       "  '        # write out the LBEP table\\n',\n",
       "  '        logger.info(f\"Writing LBEP/{self.object_type}\")\\n',\n",
       "  '        grp.create_dataset(\"LBEPR\", data=lbep_table, dtype=\"int32\")\\n',\n",
       "  '\\n',\n",
       "  '        # write out cell fates\\n',\n",
       "  '        logger.info(f\"Writing fates/{self.object_type}\")\\n',\n",
       "  '        grp.create_dataset(\"fates\", data=fate_table, dtype=\"int32\")\\n',\n",
       "  '\\n',\n",
       "  '    @property  # type: ignore\\n',\n",
       "  '    @h5check_property_exists(\"tracks\")\\n',\n",
       "  '    def lbep(self) -> np.ndarray:\\n',\n",
       "  '        \"\"\"Return the LBEP data.\"\"\"\\n',\n",
       "  '        logger.info(f\"Loading LBEP/{self.object_type}\")\\n',\n",
       "  '        return self._hdf[\"tracks\"][self.object_type][\"LBEPR\"][:]\\n',\n",
       "  '\\n',\n",
       "  '\\n',\n",
       "  'if __name__ == \"__main__\":\\n',\n",
       "  '    pass\\n'],\n",
       " 0)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import inspect\n",
    "inspect.getsourcelines(btrack.dataio) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7319e015",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>t</th>\n",
       "      <th>x</th>\n",
       "      <th>y</th>\n",
       "      <th>z</th>\n",
       "      <th>parent</th>\n",
       "      <th>root</th>\n",
       "      <th>state</th>\n",
       "      <th>generation</th>\n",
       "      <th>dummy</th>\n",
       "      <th>mean_intensity-1</th>\n",
       "      <th>mean_intensity-3</th>\n",
       "      <th>orientation</th>\n",
       "      <th>mean_intensity-2</th>\n",
       "      <th>eccentricity</th>\n",
       "      <th>axis_minor_length</th>\n",
       "      <th>mean_intensity-0</th>\n",
       "      <th>axis_major_length</th>\n",
       "      <th>area</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>1881.116694</td>\n",
       "      <td>1398.286872</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td>115.873582</td>\n",
       "      <td>129.222042</td>\n",
       "      <td>-1.419839</td>\n",
       "      <td>115.439222</td>\n",
       "      <td>0.557116</td>\n",
       "      <td>25.672430</td>\n",
       "      <td>114.231767</td>\n",
       "      <td>30.914445</td>\n",
       "      <td>617.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>1881.116694</td>\n",
       "      <td>1398.286872</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>True</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>1902.144385</td>\n",
       "      <td>1427.008913</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td>126.702317</td>\n",
       "      <td>132.212121</td>\n",
       "      <td>0.001484</td>\n",
       "      <td>119.775401</td>\n",
       "      <td>0.524600</td>\n",
       "      <td>24.932157</td>\n",
       "      <td>114.181818</td>\n",
       "      <td>29.285477</td>\n",
       "      <td>561.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>1921.829031</td>\n",
       "      <td>1452.464321</td>\n",
       "      <td>3.913878</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>True</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3</td>\n",
       "      <td>5</td>\n",
       "      <td>1949.364516</td>\n",
       "      <td>1502.100000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td>123.306452</td>\n",
       "      <td>142.632258</td>\n",
       "      <td>-1.175582</td>\n",
       "      <td>121.003226</td>\n",
       "      <td>0.286746</td>\n",
       "      <td>19.490489</td>\n",
       "      <td>111.670968</td>\n",
       "      <td>20.344835</td>\n",
       "      <td>310.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>3</td>\n",
       "      <td>6</td>\n",
       "      <td>1950.565789</td>\n",
       "      <td>1502.868421</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td>125.842105</td>\n",
       "      <td>150.046053</td>\n",
       "      <td>0.911112</td>\n",
       "      <td>123.006579</td>\n",
       "      <td>0.567245</td>\n",
       "      <td>17.867698</td>\n",
       "      <td>111.911184</td>\n",
       "      <td>21.695965</td>\n",
       "      <td>304.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>3</td>\n",
       "      <td>7</td>\n",
       "      <td>1950.313783</td>\n",
       "      <td>1501.498534</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td>129.697947</td>\n",
       "      <td>159.407625</td>\n",
       "      <td>0.982844</td>\n",
       "      <td>127.190616</td>\n",
       "      <td>0.347162</td>\n",
       "      <td>20.193609</td>\n",
       "      <td>112.571848</td>\n",
       "      <td>21.532842</td>\n",
       "      <td>341.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>3</td>\n",
       "      <td>8</td>\n",
       "      <td>1949.082707</td>\n",
       "      <td>1501.135338</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td>134.740602</td>\n",
       "      <td>159.650376</td>\n",
       "      <td>0.203720</td>\n",
       "      <td>132.263158</td>\n",
       "      <td>0.484165</td>\n",
       "      <td>17.276162</td>\n",
       "      <td>113.300752</td>\n",
       "      <td>19.744708</td>\n",
       "      <td>266.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>3</td>\n",
       "      <td>9</td>\n",
       "      <td>1949.546053</td>\n",
       "      <td>1500.888158</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td>143.628289</td>\n",
       "      <td>179.509868</td>\n",
       "      <td>-0.052578</td>\n",
       "      <td>140.437500</td>\n",
       "      <td>0.339176</td>\n",
       "      <td>19.129557</td>\n",
       "      <td>114.161184</td>\n",
       "      <td>20.334950</td>\n",
       "      <td>304.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>3</td>\n",
       "      <td>10</td>\n",
       "      <td>1950.037500</td>\n",
       "      <td>1501.360000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td>160.435000</td>\n",
       "      <td>243.707500</td>\n",
       "      <td>0.137396</td>\n",
       "      <td>155.027500</td>\n",
       "      <td>0.309445</td>\n",
       "      <td>22.110774</td>\n",
       "      <td>116.652500</td>\n",
       "      <td>23.252045</td>\n",
       "      <td>400.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>3</td>\n",
       "      <td>11</td>\n",
       "      <td>1950.579909</td>\n",
       "      <td>1501.705479</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td>187.271689</td>\n",
       "      <td>317.075342</td>\n",
       "      <td>0.135802</td>\n",
       "      <td>171.707763</td>\n",
       "      <td>0.390464</td>\n",
       "      <td>22.847189</td>\n",
       "      <td>120.073059</td>\n",
       "      <td>24.817230</td>\n",
       "      <td>438.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>3</td>\n",
       "      <td>12</td>\n",
       "      <td>1950.596244</td>\n",
       "      <td>1501.415493</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td>222.420188</td>\n",
       "      <td>380.314554</td>\n",
       "      <td>0.277715</td>\n",
       "      <td>197.884977</td>\n",
       "      <td>0.392098</td>\n",
       "      <td>22.479895</td>\n",
       "      <td>125.007042</td>\n",
       "      <td>24.436702</td>\n",
       "      <td>426.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>3</td>\n",
       "      <td>13</td>\n",
       "      <td>1950.534884</td>\n",
       "      <td>1501.695349</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td>269.093023</td>\n",
       "      <td>456.202326</td>\n",
       "      <td>0.285023</td>\n",
       "      <td>228.813953</td>\n",
       "      <td>0.377002</td>\n",
       "      <td>22.644240</td>\n",
       "      <td>130.804651</td>\n",
       "      <td>24.448217</td>\n",
       "      <td>430.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>3</td>\n",
       "      <td>14</td>\n",
       "      <td>1949.478873</td>\n",
       "      <td>1500.513078</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td>308.843058</td>\n",
       "      <td>530.348089</td>\n",
       "      <td>0.563400</td>\n",
       "      <td>267.601610</td>\n",
       "      <td>0.401022</td>\n",
       "      <td>24.382365</td>\n",
       "      <td>140.897384</td>\n",
       "      <td>26.616311</td>\n",
       "      <td>497.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "OrderedDict([('ID', 3), ('t', [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14]), ('x', [1881.1166936790924, 1881.1166936790924, 1902.144385026738, 1921.8290310582995, 1949.3645161290322, 1950.5657894736842, 1950.3137829912023, 1949.0827067669172, 1949.546052631579, 1950.0375, 1950.579908675799, 1950.5962441314555, 1950.5348837209303, 1949.4788732394366]), ('y', [1398.2868719611022, 1398.2868719611022, 1427.0089126559715, 1452.4643206979506, 1502.1, 1502.8684210526317, 1501.4985337243402, 1501.1353383458647, 1500.8881578947369, 1501.36, 1501.7054794520548, 1501.4154929577464, 1501.6953488372094, 1500.513078470825]), ('z', [0.0, 0.0, 0.0, 3.913877520827256, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]), ('parent', 3), ('root', 3), ('state', [5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5]), ('generation', 0), ('dummy', [False, True, False, True, False, False, False, False, False, False, False, False, False, False]), ('mean_intensity-1', array([115.87358185,          nan, 126.70231729,          nan,\n",
       "       123.30645161, 125.84210526, 129.69794721, 134.7406015 ,\n",
       "       143.62828947, 160.435     , 187.2716895 , 222.42018779,\n",
       "       269.09302326, 308.84305835])), ('mean_intensity-3', array([129.22204214,          nan, 132.21212121,          nan,\n",
       "       142.63225806, 150.04605263, 159.40762463, 159.65037594,\n",
       "       179.50986842, 243.7075    , 317.07534247, 380.31455399,\n",
       "       456.20232558, 530.34808853])), ('orientation', array([-1.41983869,         nan,  0.00148363,         nan, -1.17558237,\n",
       "        0.91111233,  0.98284354,  0.20371952, -0.05257791,  0.13739631,\n",
       "        0.13580221,  0.27771505,  0.28502304,  0.56340039])), ('mean_intensity-2', array([115.43922204,          nan, 119.77540107,          nan,\n",
       "       121.00322581, 123.00657895, 127.19061584, 132.26315789,\n",
       "       140.4375    , 155.0275    , 171.70776256, 197.88497653,\n",
       "       228.81395349, 267.60160966])), ('eccentricity', array([0.55711585,        nan, 0.5246    ,        nan, 0.28674569,\n",
       "       0.56724462, 0.34716214, 0.48416481, 0.33917559, 0.30944488,\n",
       "       0.39046436, 0.39209805, 0.37700233, 0.40102172])), ('axis_minor_length', array([25.67243035,         nan, 24.93215699,         nan, 19.49048894,\n",
       "       17.867698  , 20.19360938, 17.27616221, 19.12955686, 22.1107736 ,\n",
       "       22.84718939, 22.47989495, 22.64423982, 24.38236531])), ('mean_intensity-0', array([114.23176661,          nan, 114.18181818,          nan,\n",
       "       111.67096774, 111.91118421, 112.57184751, 113.30075188,\n",
       "       114.16118421, 116.6525    , 120.07305936, 125.00704225,\n",
       "       130.80465116, 140.89738431])), ('axis_major_length', array([30.91444478,         nan, 29.28547724,         nan, 20.34483492,\n",
       "       21.695965  , 21.53284183, 19.7447085 , 20.33495003, 23.25204487,\n",
       "       24.81722966, 24.43670225, 24.44821734, 26.61631097])), ('area', array([617.,  nan, 561.,  nan, 310., 304., 341., 266., 304., 400., 438.,\n",
       "       426., 430., 497.]))])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tracks[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0d626e46",
   "metadata": {},
   "outputs": [],
   "source": [
    "btrack??"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "97a5c03d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(['from __future__ import annotations\\n',\n",
       "  '\\n',\n",
       "  'import csv\\n',\n",
       "  'import itertools\\n',\n",
       "  'import logging\\n',\n",
       "  'import os\\n',\n",
       "  'import re\\n',\n",
       "  'from functools import wraps\\n',\n",
       "  'from typing import TYPE_CHECKING, Any, Dict, List, Optional, Union\\n',\n",
       "  '\\n',\n",
       "  'import h5py\\n',\n",
       "  'import numpy as np\\n',\n",
       "  '\\n',\n",
       "  '# import core\\n',\n",
       "  'from . import btypes, constants, utils\\n',\n",
       "  '\\n',\n",
       "  'if TYPE_CHECKING:\\n',\n",
       "  '    from . import BayesianTracker\\n',\n",
       "  '\\n',\n",
       "  '# get the logger instance\\n',\n",
       "  'logger = logging.getLogger(__name__)\\n',\n",
       "  '\\n',\n",
       "  '\\n',\n",
       "  '# Choose a subset of classes/functions to document in public facing API\\n',\n",
       "  '__all__ = [\"import_CSV\"]\\n',\n",
       "  '\\n',\n",
       "  '\\n',\n",
       "  'def localizations_to_objects(\\n',\n",
       "  '    localizations: Union[\\n',\n",
       "  '        np.ndarray, List[btypes.PyTrackObject], Dict[str, Any]\\n',\n",
       "  '    ]\\n',\n",
       "  ') -> List[btypes.PyTrackObject]:\\n',\n",
       "  '    \"\"\"Take a numpy array or pandas dataframe and convert to PyTrackObjects.\\n',\n",
       "  '\\n',\n",
       "  '    Parameters\\n',\n",
       "  '    ----------\\n',\n",
       "  '    localizations : list[PyTrackObject], np.ndarray, pandas.DataFrame\\n',\n",
       "  '        A list or array of localizations.\\n',\n",
       "  '\\n',\n",
       "  '    Returns\\n',\n",
       "  '    -------\\n',\n",
       "  '    objects : list[PyTrackObject]\\n',\n",
       "  '        A list of PyTrackObject objects that represent the localizations.\\n',\n",
       "  '    \"\"\"\\n',\n",
       "  '\\n',\n",
       "  '    logger.info(f\"Objects are of type: {type(localizations)}\")\\n',\n",
       "  '\\n',\n",
       "  '    if isinstance(localizations, list):\\n',\n",
       "  '        if check_object_type(localizations):\\n',\n",
       "  '            # if these are already PyTrackObjects just silently return\\n',\n",
       "  '            return localizations\\n',\n",
       "  '\\n',\n",
       "  '    # do we have a numpy array or pandas dataframe?\\n',\n",
       "  '    if isinstance(localizations, np.ndarray):\\n',\n",
       "  '        return objects_from_array(localizations)\\n',\n",
       "  '    else:\\n',\n",
       "  '        try:\\n',\n",
       "  '            objects_dict = {\\n',\n",
       "  '                c: np.asarray(localizations[c]) for c in localizations\\n',\n",
       "  '            }\\n',\n",
       "  '        except ValueError:\\n',\n",
       "  '            logger.error(f\"Unknown localization type: {type(localizations)}\")\\n',\n",
       "  '            raise TypeError(\\n',\n",
       "  '                f\"Unknown localization type: {type(localizations)}\"\\n',\n",
       "  '            )\\n',\n",
       "  '\\n',\n",
       "  '    # how many objects are there\\n',\n",
       "  '    n_objects = objects_dict[\"t\"].shape[0]\\n',\n",
       "  '    objects_dict[\"ID\"] = np.arange(n_objects)\\n',\n",
       "  '\\n',\n",
       "  '    return objects_from_dict(objects_dict)\\n',\n",
       "  '\\n',\n",
       "  '\\n',\n",
       "  'def objects_from_dict(objects_dict: dict) -> List[btypes.PyTrackObject]:\\n',\n",
       "  '    \"\"\"Construct PyTrackObjects from a dictionary\"\"\"\\n',\n",
       "  '    # now that we have the object dictionary, convert this to objects\\n',\n",
       "  '    objects = []\\n',\n",
       "  '    n_objects = int(objects_dict[\"t\"].shape[0])\\n',\n",
       "  '\\n',\n",
       "  '    assert all([v.shape[0] == n_objects for k, v in objects_dict.items()])\\n',\n",
       "  '\\n',\n",
       "  '    for i in range(n_objects):\\n',\n",
       "  '        data = {k: v[i] for k, v in objects_dict.items()}\\n',\n",
       "  '        obj = btypes.PyTrackObject.from_dict(data)\\n',\n",
       "  '        objects.append(obj)\\n',\n",
       "  '    return objects\\n',\n",
       "  '\\n',\n",
       "  '\\n',\n",
       "  'def objects_from_array(\\n',\n",
       "  '    objects_arr: np.ndarray, default_keys=constants.DEFAULT_OBJECT_KEYS\\n',\n",
       "  ') -> List[btypes.PyTrackObject]:\\n',\n",
       "  '    \"\"\"Construct PyTrackObjects from a numpy array.\"\"\"\\n',\n",
       "  '    assert objects_arr.ndim == 2\\n',\n",
       "  '\\n',\n",
       "  '    n_features = objects_arr.shape[1]\\n',\n",
       "  '    assert n_features >= 3\\n',\n",
       "  '\\n',\n",
       "  '    n_objects = objects_arr.shape[0]\\n',\n",
       "  '\\n',\n",
       "  '    keys = default_keys[:n_features]\\n',\n",
       "  '    objects_dict = {keys[i]: objects_arr[:, i] for i in range(n_features)}\\n',\n",
       "  '    objects_dict[\"ID\"] = np.arange(n_objects)\\n',\n",
       "  '    return objects_from_dict(objects_dict)\\n',\n",
       "  '\\n',\n",
       "  '\\n',\n",
       "  'def import_JSON(filename: str):\\n',\n",
       "  '    \"\"\"Generic JSON importer for localisations from other software.\"\"\"\\n',\n",
       "  '    raise DeprecationWarning(\"`import_JSON has been deprecated`\")\\n',\n",
       "  '\\n',\n",
       "  '\\n',\n",
       "  'def import_CSV(filename: os.PathLike) -> List[btypes.PyTrackObject]:\\n',\n",
       "  '    \"\"\"Import localizations from a CSV file.\\n',\n",
       "  '\\n',\n",
       "  '    Parameters\\n',\n",
       "  '    ----------\\n',\n",
       "  '    filename : PathLike\\n',\n",
       "  '        The filename of the CSV to import\\n',\n",
       "  '\\n',\n",
       "  '    Returns\\n',\n",
       "  '    -------\\n',\n",
       "  '    objects : List[btypes.PyTrackObject]\\n',\n",
       "  '        A list of objects in the CSV file.\\n',\n",
       "  '\\n',\n",
       "  '    Notes\\n',\n",
       "  '    -----\\n',\n",
       "  '    CSV file should have one of the following format.\\n',\n",
       "  '\\n',\n",
       "  '    .. list-table:: CSV header format\\n',\n",
       "  '       :widths: 20 20 20 20 20\\n',\n",
       "  '       :header-rows: 1\\n',\n",
       "  '\\n',\n",
       "  '       * - t\\n',\n",
       "  '         - x\\n',\n",
       "  '         - y\\n',\n",
       "  '         - z\\n',\n",
       "  '         - label\\n',\n",
       "  '       * - required\\n',\n",
       "  '         - required\\n',\n",
       "  '         - required\\n',\n",
       "  '         - optional\\n',\n",
       "  '         - optional\\n',\n",
       "  '    \"\"\"\\n',\n",
       "  '\\n',\n",
       "  '    objects = []\\n',\n",
       "  '    with open(filename, \"r\") as csv_file:\\n',\n",
       "  '        csvreader = csv.DictReader(csv_file, delimiter=\",\", quotechar=\"|\")\\n',\n",
       "  '        for i, row in enumerate(csvreader):\\n',\n",
       "  '            data = {k: float(v) for k, v in row.items()}\\n',\n",
       "  '            data.update({\"ID\": i})\\n',\n",
       "  '            obj = btypes.PyTrackObject.from_dict(data)\\n',\n",
       "  '            objects.append(obj)\\n',\n",
       "  '    return objects\\n',\n",
       "  '\\n',\n",
       "  '\\n',\n",
       "  'def export_delegator(\\n',\n",
       "  '    filename: os.PathLike,\\n',\n",
       "  '    tracker: BayesianTracker,\\n',\n",
       "  '    obj_type: Optional[str] = None,\\n',\n",
       "  '    filter_by: Optional[str] = None,\\n',\n",
       "  ') -> None:\\n',\n",
       "  '    \"\"\"Export data from the tracker using the appropriate exporter.\\n',\n",
       "  '\\n',\n",
       "  '    Parameters\\n',\n",
       "  '    ----------\\n',\n",
       "  '    filename : str\\n',\n",
       "  '        The filename to export the data. The extension (e.g. .h5) is used\\n',\n",
       "  '        to select the correct export function.\\n',\n",
       "  '    tracker : BayesianTracker\\n',\n",
       "  '        An instance of the tracker.\\n',\n",
       "  '    obj_type : str, optional\\n',\n",
       "  '        The object type to export the data. Usually `obj_type_1`\\n',\n",
       "  '    filter_by : str, optional\\n',\n",
       "  '        A string that represents how the data has been filtered prior to\\n',\n",
       "  '        tracking, e.g. using the object property `area>100`\\n',\n",
       "  '\\n',\n",
       "  '    Notes\\n',\n",
       "  '    -----\\n',\n",
       "  '    This uses the appropriate exporter dependent on the given file extension.\\n',\n",
       "  '    \"\"\"\\n',\n",
       "  '    export_dir, export_fn = os.path.split(filename)\\n',\n",
       "  '    _, ext = os.path.splitext(filename)\\n',\n",
       "  '\\n',\n",
       "  '    if ext == \".csv\":\\n',\n",
       "  '        export_CSV(filename, tracker.tracks, obj_type=obj_type)\\n',\n",
       "  '    elif ext in (\".hdf\", \".hdf5\", \".h5\"):\\n',\n",
       "  '        _export_HDF(filename, tracker, obj_type=obj_type, filter_by=filter_by)\\n',\n",
       "  '    else:\\n',\n",
       "  '        logger.error(f\"Export file format {ext} not recognized.\")\\n',\n",
       "  '\\n',\n",
       "  '\\n',\n",
       "  'def check_track_type(tracks: list) -> bool:\\n',\n",
       "  '    return all(isinstance(t, btypes.Tracklet) for t in tracks)\\n',\n",
       "  '\\n',\n",
       "  '\\n',\n",
       "  'def check_object_type(objects: list) -> bool:\\n',\n",
       "  '    return all(isinstance(o, btypes.PyTrackObject) for o in objects)\\n',\n",
       "  '\\n',\n",
       "  '\\n',\n",
       "  'def export_CSV(\\n',\n",
       "  '    filename: os.PathLike,\\n',\n",
       "  '    tracks: list,\\n',\n",
       "  '    properties: list = constants.DEFAULT_EXPORT_PROPERTIES,\\n',\n",
       "  '    obj_type: Optional[str] = None,\\n',\n",
       "  '):\\n',\n",
       "  '    \"\"\"Export the track data as a simple CSV file.\\n',\n",
       "  '\\n',\n",
       "  '    Parameters\\n',\n",
       "  '    ----------\\n',\n",
       "  '    filename : str\\n',\n",
       "  '        The filename of the file to be exported.\\n',\n",
       "  '    tracks : list[Tracklet]\\n',\n",
       "  '        A list of Tracklet objects to be exported.\\n',\n",
       "  '    properties : list, default = constants.DEFAULT_EXPORT_PROPERTIES\\n',\n",
       "  '        A list of tracklet properties to be exported.\\n',\n",
       "  '    obj_type : str, optional\\n',\n",
       "  '        A string describing the object type, e.g. `obj_type_1`.\\n',\n",
       "  '\\n',\n",
       "  '    \"\"\"\\n',\n",
       "  '\\n',\n",
       "  '    if not tracks:\\n',\n",
       "  '        logger.error(f\"No tracks found when exporting to: {filename}\")\\n',\n",
       "  '        return\\n',\n",
       "  '\\n',\n",
       "  '    if not check_track_type(tracks):\\n',\n",
       "  '        logger.error(\"Tracks of incorrect type\")\\n',\n",
       "  '\\n',\n",
       "  '    logger.info(f\"Writing out CSV files to: {filename}\")\\n',\n",
       "  '    export_track = np.vstack([t.to_array(properties) for t in tracks])\\n',\n",
       "  '\\n',\n",
       "  '    with open(filename, \"w\", newline=\"\") as csv_file:\\n',\n",
       "  '        csvwriter = csv.writer(csv_file, delimiter=\" \")\\n',\n",
       "  '        csvwriter.writerow(properties)\\n',\n",
       "  '        for i in range(export_track.shape[0]):\\n',\n",
       "  '            csvwriter.writerow(export_track[i, :].tolist())\\n',\n",
       "  '\\n',\n",
       "  '\\n',\n",
       "  'def export_LBEP(filename: os.PathLike, tracks: list):\\n',\n",
       "  '    \"\"\"Export the LBEP table as a text file.\"\"\"\\n',\n",
       "  '    if not tracks:\\n',\n",
       "  '        logger.error(f\"No tracks found when exporting to: {filename}\")\\n',\n",
       "  '        return\\n',\n",
       "  '\\n',\n",
       "  '    if not check_track_type(tracks):\\n',\n",
       "  '        logger.error(\"Tracks of incorrect type\")\\n',\n",
       "  '\\n',\n",
       "  '    tracks.sort(key=lambda t: t.ID)\\n',\n",
       "  '\\n',\n",
       "  '    with open(filename, \"w\") as lbep_file:\\n',\n",
       "  '        logger.info(f\"Writing LBEP file: {filename}...\")\\n',\n",
       "  '        for track in tracks:\\n',\n",
       "  '            lbep = f\"{track.ID} {track.t[0]} {track.t[-1]} {track.parent}\"\\n',\n",
       "  '            lbep_file.write(f\"{lbep}\\\\n\")\\n',\n",
       "  '\\n',\n",
       "  '\\n',\n",
       "  'def _export_HDF(\\n',\n",
       "  '    filename: os.PathLike, tracker, obj_type=None, filter_by: str = None\\n',\n",
       "  '):\\n',\n",
       "  '    \"\"\"Export to HDF.\"\"\"\\n',\n",
       "  '\\n',\n",
       "  '    filename_noext, ext = os.path.splitext(filename)\\n',\n",
       "  '    if not ext == \".h5\":\\n',\n",
       "  '        filename = filename_noext + \".h5\"\\n',\n",
       "  '        logger.warning(f\"Changing HDF filename to {filename}\")\\n',\n",
       "  '\\n',\n",
       "  '    with HDF5FileHandler(filename, read_write=\"a\", obj_type=obj_type) as hdf:\\n',\n",
       "  '        # if there are no objects, write them out\\n',\n",
       "  '        if f\"objects/{obj_type}\" not in hdf._hdf:\\n',\n",
       "  '            hdf.write_objects(tracker)\\n',\n",
       "  '        # write the tracks\\n',\n",
       "  '        hdf.write_tracks(tracker, f_expr=filter_by)\\n',\n",
       "  '\\n',\n",
       "  '\\n',\n",
       "  'def h5check_property_exists(property):\\n',\n",
       "  '    \"\"\"Wrapper for hdf handler to make sure a property exists.\"\"\"\\n',\n",
       "  '\\n',\n",
       "  '    def func(fn):\\n',\n",
       "  '        @wraps(fn)\\n',\n",
       "  '        def wrapped_handler_property(*args, **kwargs):\\n',\n",
       "  '            self = args[0]\\n',\n",
       "  '            assert isinstance(self, HDF5FileHandler)\\n',\n",
       "  '            if property not in self._hdf:\\n',\n",
       "  '                logger.error(\\n',\n",
       "  '                    f\"{property.capitalize()} not found in {self.filename}\"\\n',\n",
       "  '                )\\n',\n",
       "  '                return None\\n',\n",
       "  '            return fn(*args, **kwargs)\\n',\n",
       "  '\\n',\n",
       "  '        return wrapped_handler_property\\n',\n",
       "  '\\n',\n",
       "  '    return func\\n',\n",
       "  '\\n',\n",
       "  '\\n',\n",
       "  'class HDF5FileHandler:\\n',\n",
       "  '    \"\"\"Generic HDF5 file hander for reading and writing datasets. This is\\n',\n",
       "  '    inter-operable between segmentation, tracking and analysis code.\\n',\n",
       "  '\\n',\n",
       "  '    Parameters\\n',\n",
       "  '    ----------\\n',\n",
       "  '    filename : str\\n',\n",
       "  '        The filename of the hdf5 file to be used.\\n',\n",
       "  '    read_write : str\\n',\n",
       "  '        A read/write mode for the file, e.g. `w`, `r`, `a` etc.\\n',\n",
       "  '    obj_type : str\\n',\n",
       "  '        The name of the object type. Defaults to `obj_type_1`. The object type\\n',\n",
       "  '        name must start with `obj_type_`\\n',\n",
       "  '\\n',\n",
       "  '    Attributes\\n',\n",
       "  '    ----------\\n',\n",
       "  '    segmentation : np.ndarray\\n',\n",
       "  '        A numpy array representing the segmentation data. TZYX\\n',\n",
       "  '    objects : list [PyTrackObject]\\n',\n",
       "  '        A list of PyTrackObjects localised from the segmentation data.\\n',\n",
       "  '    filtered_objects  : np.ndarray\\n',\n",
       "  '        Similar to objects, but filtered by property.\\n',\n",
       "  '    tracks : list [Tracklet]\\n',\n",
       "  '        A list of Tracklet objects.\\n',\n",
       "  '    lbep : np.ndarray\\n',\n",
       "  '        The LBEP table representing the track graph.\\n',\n",
       "  '\\n',\n",
       "  '    Notes\\n',\n",
       "  '    -----\\n',\n",
       "  '    ```\\n',\n",
       "  '    Basic format of the HDF file is:\\n',\n",
       "  '        segmentation/\\n',\n",
       "  '            images          - (J x (d) x h x w) uint16 segmentation\\n',\n",
       "  '        objects/\\n',\n",
       "  '            obj_type_1/\\n',\n",
       "  '                coords      - (I x 5) [t, x, y, z, object_type]\\n',\n",
       "  '                labels      - (I x D) [label, (softmax scores ...)]\\n',\n",
       "  '                map         - (J x 2) [start_index, end_index] -> coords array\\n',\n",
       "  '                properties/\\n',\n",
       "  '                    area  - (I x 1) first named property (e.g. `area`)\\n',\n",
       "  '                    ...\\n',\n",
       "  '            ...\\n',\n",
       "  '        tracks/\\n',\n",
       "  '            obj_type_1/\\n',\n",
       "  '                tracks      - (I x 1) [index into coords]\\n',\n",
       "  '                dummies     - similar to coords, but for dummy objects\\n',\n",
       "  '                map         - (K x 2) [start_index, end_index] -> tracks array\\n',\n",
       "  '                LBEPRG      - (K x 6) [L, B, E, P, R, G]\\n',\n",
       "  '                fates       - (K x n) [fate_from_tracker, ...future_expansion]\\n',\n",
       "  '            ...\\n',\n",
       "  '\\n',\n",
       "  '    Where:\\n',\n",
       "  '        I - number of objects\\n',\n",
       "  '        J - number of frames\\n',\n",
       "  '        K - number of tracks\\n',\n",
       "  '    ```\\n',\n",
       "  '\\n',\n",
       "  '    LBEPR is a modification of the LBEP format to also include the root node\\n',\n",
       "  '    of the tree.\\n',\n",
       "  '\\n',\n",
       "  '\\n',\n",
       "  '    Examples\\n',\n",
       "  '    --------\\n',\n",
       "  '    Read objects from a file:\\n',\n",
       "  \"    >>> with HDF5FileHandler('file.h5', 'r') as handler:\\n\",\n",
       "  '    >>>    objects = handler.objects\\n',\n",
       "  '\\n',\n",
       "  '    Use filtering by property for object retrieval:\\n',\n",
       "  \"    >>> obj = handler.filtered_objects('flag==1')\\n\",\n",
       "  \"    >>> obj = handler.filtered_objects('area>100')\\n\",\n",
       "  '\\n',\n",
       "  '    Write tracks directly to a file:\\n',\n",
       "  '    >>> handler.write_tracks(tracks)\\n',\n",
       "  '    \"\"\"\\n',\n",
       "  '\\n',\n",
       "  '    def __init__(\\n',\n",
       "  '        self,\\n',\n",
       "  '        filename: os.PathLike,\\n',\n",
       "  '        read_write: str = \"r\",\\n',\n",
       "  '        *,\\n',\n",
       "  '        obj_type: str = \"obj_type_1\",\\n',\n",
       "  '    ):\\n',\n",
       "  '\\n',\n",
       "  '        self._f_expr = None  # DO NOT USE\\n',\n",
       "  '        self.object_type = obj_type\\n',\n",
       "  '\\n',\n",
       "  '        self.filename = filename\\n',\n",
       "  '        logger.info(f\"Opening HDF file: {self.filename}...\")\\n',\n",
       "  '        self._hdf = h5py.File(filename, read_write)\\n',\n",
       "  '        self._states = list(constants.States)\\n',\n",
       "  '\\n',\n",
       "  '    @property\\n',\n",
       "  '    def object_types(self) -> List[str]:\\n',\n",
       "  '        return list(self._hdf[\"objects\"].keys())\\n',\n",
       "  '\\n',\n",
       "  '    def __enter__(self):\\n',\n",
       "  '        return self\\n',\n",
       "  '\\n',\n",
       "  '    def __exit__(self, exc_type, exc_value, traceback):\\n',\n",
       "  '        self.close()\\n',\n",
       "  '\\n',\n",
       "  '    def close(self):\\n',\n",
       "  '        if not self._hdf:\\n',\n",
       "  '            return\\n',\n",
       "  '        logger.info(f\"Closing HDF file: {self.filename}\")\\n',\n",
       "  '        self._hdf.close()\\n',\n",
       "  '\\n',\n",
       "  '    @property\\n',\n",
       "  '    def object_type(self) -> str:\\n',\n",
       "  '        return self._object_type\\n',\n",
       "  '\\n',\n",
       "  '    @object_type.setter\\n',\n",
       "  '    def object_type(self, obj_type: str) -> None:\\n',\n",
       "  '        if not obj_type.startswith(\"obj_type_\"):\\n',\n",
       "  '            raise ValueError(\"Object type must start with ``obj_type_``\")\\n',\n",
       "  '        self._object_type = obj_type\\n',\n",
       "  '\\n',\n",
       "  '    @property  # type: ignore\\n',\n",
       "  '    @h5check_property_exists(\"segmentation\")\\n',\n",
       "  '    def segmentation(self) -> np.ndarray:\\n',\n",
       "  '        segmentation = self._hdf[\"segmentation\"][\"images\"][:].astype(np.uint16)\\n',\n",
       "  '        logger.info(f\"Loading segmentation {segmentation.shape}\")\\n',\n",
       "  '        return segmentation\\n',\n",
       "  '\\n',\n",
       "  '    def write_segmentation(self, segmentation: np.ndarray) -> None:\\n',\n",
       "  '        \"\"\"Write out the segmentation to an HDF file.\\n',\n",
       "  '\\n',\n",
       "  '        Parameters\\n',\n",
       "  '        ----------\\n',\n",
       "  '        segmentation : np.ndarray\\n',\n",
       "  '            A numpy array representing the segmentation data. T(Z)YX, uint16\\n',\n",
       "  '        \"\"\"\\n',\n",
       "  '        # write the segmentation out\\n',\n",
       "  '        grp = self._hdf.create_group(\"segmentation\")\\n',\n",
       "  '        grp.create_dataset(\\n',\n",
       "  '            \"images\",\\n',\n",
       "  '            data=segmentation,\\n',\n",
       "  '            dtype=\"uint16\",\\n',\n",
       "  '            compression=\"gzip\",\\n',\n",
       "  '            compression_opts=7,\\n',\n",
       "  '        )\\n',\n",
       "  '\\n',\n",
       "  '    @property\\n',\n",
       "  '    def objects(self) -> List[btypes.PyTrackObject]:\\n',\n",
       "  '        \"\"\"Return the objects in the file.\"\"\"\\n',\n",
       "  '        return self.filtered_objects()\\n',\n",
       "  '\\n',\n",
       "  '    @h5check_property_exists(\"objects\")\\n',\n",
       "  '    def filtered_objects(\\n',\n",
       "  '        self, f_expr: Optional[str] = None\\n',\n",
       "  '    ) -> List[btypes.PyTrackObject]:\\n',\n",
       "  '        \"\"\"A filtered list of objects based on metadata. f_expr should be of the\\n',\n",
       "  '        format `flag==1`.\"\"\"\\n',\n",
       "  '\\n',\n",
       "  '        if self.object_type not in self.object_types:\\n',\n",
       "  '            raise ValueError(f\"Object type {self.object_type} not recognized\")\\n',\n",
       "  '\\n',\n",
       "  '        grp = self._hdf[\"objects\"][self.object_type]\\n',\n",
       "  '\\n',\n",
       "  '        # read the whole dataset into memory\\n',\n",
       "  '        txyz = grp[\"coords\"][:]\\n',\n",
       "  '        if \"labels\" not in grp:\\n',\n",
       "  '            logger.warning(\"Labels missing from objects in HDF file\")\\n',\n",
       "  '            labels = np.zeros((txyz.shape[0], 6))\\n',\n",
       "  '        else:\\n',\n",
       "  '            labels = self._hdf[\"objects\"][self.object_type][\"labels\"][:]\\n',\n",
       "  '\\n',\n",
       "  '        # get properties if we have them (note, this assumes that the same\\n',\n",
       "  '        # properties exist for each object)\\n',\n",
       "  '        properties = {}\\n',\n",
       "  '        if \"properties\" in grp:\\n',\n",
       "  '            properties = {\\n',\n",
       "  '                k: grp[\"properties\"][k][:] for k in grp[\"properties\"]\\n',\n",
       "  '            }\\n',\n",
       "  '            assert all([len(p) == len(txyz) for p in properties.values()])\\n',\n",
       "  '\\n',\n",
       "  \"        # note that this doesn't do much error checking at the moment\\n\",\n",
       "  '        # TODO(arl): this should now reference the `properties`\\n',\n",
       "  '        if f_expr is not None:\\n',\n",
       "  '            assert isinstance(f_expr, str)\\n',\n",
       "  '            pattern = r\"(?P<name>\\\\w+)(?P<op>[\\\\>\\\\<\\\\=]+)(?P<cmp>[0-9]+)\"\\n',\n",
       "  '            m = re.match(pattern, f_expr)\\n',\n",
       "  '\\n',\n",
       "  '            if m is None:\\n',\n",
       "  '                raise ValueError(f\"Cannot filter objects by {f_expr}\")\\n',\n",
       "  '\\n',\n",
       "  '            f_eval = f\"x{m[\\'op\\']}{m[\\'cmp\\']}\"  # e.g. x > 10\\n',\n",
       "  '\\n',\n",
       "  '            # old files have these stored differently\\n',\n",
       "  '            if \"properties\" in grp.keys():\\n',\n",
       "  '                property_group = grp[\"properties\"]\\n',\n",
       "  '            else:\\n',\n",
       "  '                property_group = grp\\n',\n",
       "  '\\n',\n",
       "  '            if m[\"name\"] in property_group.keys():\\n',\n",
       "  '                # logger.info(f\"Property {m[\\'name\\']} found in {property_group}.\")\\n',\n",
       "  '                data = property_group[m[\"name\"]][:]\\n',\n",
       "  '                filtered_idx = [i for i, x in enumerate(data) if eval(f_eval)]\\n',\n",
       "  '            else:\\n',\n",
       "  '                raise ValueError(f\"Cannot filter objects by {f_expr}\")\\n',\n",
       "  '\\n',\n",
       "  '        else:\\n',\n",
       "  '            filtered_idx = range(txyz.shape[0])  # default filtering uses all\\n',\n",
       "  '\\n',\n",
       "  '        # sanity check that coordinates matches labels\\n',\n",
       "  '        assert txyz.shape[0] == labels.shape[0]\\n',\n",
       "  '        logger.info(\\n',\n",
       "  '            f\"Loading objects/{self.object_type} {txyz.shape} \"\\n',\n",
       "  '            f\"({len(filtered_idx)} filtered: {f_expr})\"\\n',\n",
       "  '        )\\n',\n",
       "  '\\n',\n",
       "  '        txyz_filtered = txyz[filtered_idx, :]\\n',\n",
       "  '        labels_filtered = labels[filtered_idx, :]\\n',\n",
       "  '\\n',\n",
       "  '        objects_dict = {\\n',\n",
       "  '            \"t\": txyz_filtered[:, 0],\\n',\n",
       "  '            \"x\": txyz_filtered[:, 1],\\n',\n",
       "  '            \"y\": txyz_filtered[:, 2],\\n',\n",
       "  '            \"z\": txyz_filtered[:, 3],\\n',\n",
       "  '            \"label\": labels_filtered[:, 0],\\n',\n",
       "  '            \"ID\": np.asarray(filtered_idx),\\n',\n",
       "  '        }\\n',\n",
       "  '\\n',\n",
       "  '        # add the filtered properties\\n',\n",
       "  '        for key, props in properties.items():\\n',\n",
       "  '            objects_dict.update({key: props[filtered_idx]})\\n',\n",
       "  '\\n',\n",
       "  '        return objects_from_dict(objects_dict)\\n',\n",
       "  '\\n',\n",
       "  '    def write_objects(\\n',\n",
       "  '        self, data: Union[List[btypes.PyTrackObject, BayesianTracker]]\\n',\n",
       "  '    ) -> None:\\n',\n",
       "  '        \"\"\"Write objects to HDF file.\\n',\n",
       "  '\\n',\n",
       "  '        Parameters\\n',\n",
       "  '        ----------\\n',\n",
       "  '        data : list or BayesianTracker instance\\n',\n",
       "  '            Either a list of PyTrackObject to be written, or an instance of\\n',\n",
       "  '            BayesianTracker with a .objects property.\\n',\n",
       "  '        \"\"\"\\n',\n",
       "  '        # TODO(arl): make sure that the objects are ordered in time\\n',\n",
       "  '\\n',\n",
       "  '        if isinstance(data, list):\\n',\n",
       "  '            objects = data\\n',\n",
       "  '        elif hasattr(data, \"objects\"):\\n',\n",
       "  '            objects = data.objects\\n',\n",
       "  '        else:\\n',\n",
       "  '            raise TypeError(\"Object type not recognized.\")\\n',\n",
       "  '\\n',\n",
       "  '        # make sure that the data to be written are all of type PyTrackObject\\n',\n",
       "  '        if not check_object_type(objects):\\n',\n",
       "  '            raise TypeError(\"Object type not recognized.\")\\n',\n",
       "  '\\n',\n",
       "  '        if \"objects\" not in self._hdf:\\n',\n",
       "  '            self._hdf.create_group(\"objects\")\\n',\n",
       "  '        grp = self._hdf[\"objects\"].create_group(self.object_type)\\n',\n",
       "  '        props = {k: [] for k in objects[0].properties.keys()}\\n',\n",
       "  '\\n',\n",
       "  '        n_objects = len(objects)\\n',\n",
       "  '        n_frames = np.max([o.t for o in objects]) + 1\\n',\n",
       "  '\\n',\n",
       "  '        txyz = np.zeros((n_objects, 5), dtype=np.float32)\\n',\n",
       "  '        labels = np.zeros((n_objects, 1), dtype=np.uint8)\\n',\n",
       "  '        fmap = np.zeros((n_frames, 2), dtype=np.uint32)\\n',\n",
       "  '\\n',\n",
       "  '        # convert the btrack objects into a numpy array\\n',\n",
       "  '        for i, obj in enumerate(objects):\\n',\n",
       "  '            txyz[i, :] = [obj.t, obj.x, obj.y, obj.z, 0]\\n',\n",
       "  '            labels[i, :] = obj.label\\n',\n",
       "  '            t = int(obj.t)\\n',\n",
       "  '            fmap[t, 1] = np.max([fmap[t, 1], i])\\n',\n",
       "  '\\n',\n",
       "  '            # add in any properties\\n',\n",
       "  '            for key in props.keys():\\n',\n",
       "  '                props[key].append(obj.properties[key])\\n',\n",
       "  '\\n',\n",
       "  '        fmap[1:, 0] = fmap[:-1, 1]\\n',\n",
       "  '\\n',\n",
       "  '        logger.info(f\"Writing objects/{self.object_type}\")\\n',\n",
       "  '        grp.create_dataset(\"coords\", data=txyz, dtype=\"float32\")\\n',\n",
       "  '        grp.create_dataset(\"map\", data=fmap, dtype=\"uint32\")\\n',\n",
       "  '\\n',\n",
       "  '        logger.info(f\"Writing labels/{self.object_type}\")\\n',\n",
       "  '        grp.create_dataset(\"labels\", data=labels, dtype=\"float32\")\\n',\n",
       "  '\\n',\n",
       "  '        # finally, write any properties\\n',\n",
       "  '        self.write_properties(props)\\n',\n",
       "  '\\n',\n",
       "  '    @h5check_property_exists(\"objects\")\\n',\n",
       "  '    def write_properties(\\n',\n",
       "  '        self, data: Dict[str, Any], *, allow_overwrite: bool = False\\n',\n",
       "  '    ) -> None:\\n',\n",
       "  '        \"\"\"Write object properties to HDF file.\\n',\n",
       "  '\\n',\n",
       "  '        Parameters\\n',\n",
       "  '        ----------\\n',\n",
       "  '        data : dict {key: (N, D)}\\n',\n",
       "  '            A dictionary of key-value pairs of properties to be written. The\\n',\n",
       "  '            values should be an array equal in length to the number of objects\\n',\n",
       "  '            and with D dimensions.\\n',\n",
       "  '        allow_overwrite : bool\\n',\n",
       "  '            Allow to delete the existing property keys from the HDF5 file and\\n',\n",
       "  '            overwrite with new values from the data dict. Defaults to False.\\n',\n",
       "  '        \"\"\"\\n',\n",
       "  '\\n',\n",
       "  '        if not isinstance(data, dict):\\n',\n",
       "  '            raise TypeError(\"Properties must be a dictionary.\")\\n',\n",
       "  '\\n',\n",
       "  '        grp = self._hdf[f\"objects/{self.object_type}\"]\\n',\n",
       "  '\\n',\n",
       "  '        if \"properties\" not in grp.keys():\\n',\n",
       "  '            props_grp = grp.create_group(\"properties\")\\n',\n",
       "  '        else:\\n',\n",
       "  '            props_grp = self._hdf[f\"objects/{self.object_type}/properties\"]\\n',\n",
       "  '\\n',\n",
       "  '        n_objects = len(self.objects)\\n',\n",
       "  '\\n',\n",
       "  '        for key, values in data.items():\\n',\n",
       "  '            # Manage the property data:\\n',\n",
       "  '            if not values:\\n',\n",
       "  '                logger.warning(f\"Property \\'{key}\\' is empty.\")\\n',\n",
       "  '                continue\\n',\n",
       "  '            values = np.array(values)\\n',\n",
       "  '            assert values.shape[0] == n_objects\\n',\n",
       "  '\\n',\n",
       "  '            # Check if the property is already in the props_grp:\\n',\n",
       "  '            if key in props_grp:\\n',\n",
       "  '                if allow_overwrite is False:\\n',\n",
       "  '                    logger.info(\\n',\n",
       "  '                        f\"Property \\'{key}\\' already written in the file\"\\n',\n",
       "  '                    )\\n',\n",
       "  '                    raise KeyError(\\n',\n",
       "  '                        f\"Property \\'{key}\\' already in file -> switch on \"\\n',\n",
       "  '                        \"\\'overwrite\\' param to replace existing property \"\\n',\n",
       "  '                    )\\n',\n",
       "  '                else:\\n',\n",
       "  '                    del self._hdf[f\"objects/{self.object_type}/properties\"][\\n',\n",
       "  '                        key\\n',\n",
       "  '                    ]\\n',\n",
       "  '                    logger.info(\\n',\n",
       "  '                        f\"Property \\'{key}\\' erased to be overwritten...\"\\n',\n",
       "  '                    )\\n',\n",
       "  '\\n',\n",
       "  '            # Now that you handled overwriting, write the values:\\n',\n",
       "  '            logger.info(\\n',\n",
       "  '                f\"Writing properties/{self.object_type}/{key} {values.shape}\"\\n',\n",
       "  '            )\\n',\n",
       "  '            props_grp.create_dataset(key, data=data[key], dtype=\"float32\")\\n',\n",
       "  '\\n',\n",
       "  '    @property  # type: ignore\\n',\n",
       "  '    @h5check_property_exists(\"tracks\")\\n',\n",
       "  '    def tracks(self) -> List[btypes.Tracklet]:\\n',\n",
       "  '        \"\"\"Return the tracks in the file.\"\"\"\\n',\n",
       "  '\\n',\n",
       "  '        logger.info(f\"Loading tracks/{self.object_type}\")\\n',\n",
       "  '        track_map = self._hdf[\"tracks\"][self.object_type][\"map\"][:]\\n',\n",
       "  '        track_refs = self._hdf[\"tracks\"][self.object_type][\"tracks\"][:]\\n',\n",
       "  '        lbep = self.lbep\\n',\n",
       "  '        fates = self._hdf[\"tracks\"][self.object_type][\"fates\"][:]\\n',\n",
       "  '\\n',\n",
       "  '        # if there are dummies, make new dummy objects\\n',\n",
       "  '        if \"dummies\" in self._hdf[\"tracks\"][self.object_type]:\\n',\n",
       "  '            dummies = self._hdf[\"tracks\"][self.object_type][\"dummies\"][:]\\n',\n",
       "  '            dummy_obj = objects_from_array(dummies[:, :4])\\n',\n",
       "  '            for d in dummy_obj:\\n',\n",
       "  '                d.ID = -(d.ID + 1)  # restore the -ve ID\\n',\n",
       "  '                d.dummy = True  # set the dummy flag to true\\n',\n",
       "  '\\n',\n",
       "  '        # TODO(arl): this needs to be stored in the HDF folder\\n',\n",
       "  '        if (\\n',\n",
       "  '            \"f_expr\" in self._hdf[\"tracks\"][self.object_type].attrs\\n',\n",
       "  '            and self._f_expr is None\\n',\n",
       "  '        ):\\n',\n",
       "  '            f_expr = self._hdf[\"tracks\"][self.object_type].attrs[\"f_expr\"]\\n',\n",
       "  '        elif self._f_expr is not None:\\n',\n",
       "  '            f_expr = self._f_expr\\n',\n",
       "  '        else:\\n',\n",
       "  '            f_expr = None\\n',\n",
       "  '\\n',\n",
       "  '        obj = self.filtered_objects(f_expr=f_expr)\\n',\n",
       "  '\\n',\n",
       "  '        def _get_txyz(_ref: int) -> int:\\n',\n",
       "  '            if _ref >= 0:\\n',\n",
       "  '                return obj[_ref]\\n',\n",
       "  '            return dummy_obj[abs(_ref) - 1]  # references are -ve for dummies\\n',\n",
       "  '\\n',\n",
       "  '        tracks = []\\n',\n",
       "  '        for i in range(track_map.shape[0]):\\n',\n",
       "  '            idx = slice(*track_map[i, :].tolist())\\n',\n",
       "  '            refs = track_refs[idx]\\n',\n",
       "  '            track = btypes.Tracklet(lbep[i, 0], list(map(_get_txyz, refs)))\\n',\n",
       "  '            track.parent = lbep[i, 3]  # set the parent and root of tree\\n',\n",
       "  '            track.root = lbep[i, 4]\\n',\n",
       "  '            if lbep.shape[1] > 5:\\n',\n",
       "  '                track.generation = lbep[i, 5]\\n',\n",
       "  '            track.fate = constants.Fates(fates[i])  # restore the track fate\\n',\n",
       "  '            tracks.append(track)\\n',\n",
       "  '\\n',\n",
       "  '        # once we have all of the tracks, populate the children\\n',\n",
       "  '        to_update = {}\\n',\n",
       "  '        for track in tracks:\\n',\n",
       "  '            if not track.is_root:\\n',\n",
       "  '                parents = filter(lambda t: t.ID == track.parent, tracks)\\n',\n",
       "  '                for parent in parents:\\n',\n",
       "  '                    if parent not in to_update:\\n',\n",
       "  '                        to_update[parent] = []\\n',\n",
       "  '                    to_update[parent].append(track.ID)\\n',\n",
       "  '\\n',\n",
       "  '        # sanity check, can be removed at a later date\\n',\n",
       "  '        assert all([len(children) <= 2 for children in to_update.values()])\\n',\n",
       "  '\\n',\n",
       "  '        # add the children to the parent\\n',\n",
       "  '        for track, children in to_update.items():\\n',\n",
       "  '            track.children = children\\n',\n",
       "  '\\n',\n",
       "  '        return tracks\\n',\n",
       "  '\\n',\n",
       "  '    def write_tracks(\\n',\n",
       "  '        self,\\n',\n",
       "  '        data: Union[List[btypes.Tracklet], BayesianTracker],\\n',\n",
       "  '        *,\\n',\n",
       "  '        f_expr: Optional[str] = None,\\n',\n",
       "  '    ) -> None:\\n',\n",
       "  '        \"\"\"Write tracks to HDF file.\\n',\n",
       "  '\\n',\n",
       "  '        Parameters\\n',\n",
       "  '        ----------\\n',\n",
       "  '        data : list of Tracklets or an instance of BayesianTracker\\n',\n",
       "  '            A list of tracklets or an instance of BayesianTracker.\\n',\n",
       "  '        f_expr : str\\n',\n",
       "  '            An expression which represents how the objects have been filtered\\n',\n",
       "  '            prior to tracking, e.g. `area>100.0`\\n',\n",
       "  '        \"\"\"\\n',\n",
       "  '\\n',\n",
       "  '        if isinstance(data, list):\\n',\n",
       "  '            if not check_track_type(data):\\n',\n",
       "  '                raise ValueError(f\"Data of type {type(data)} not supported.\")\\n',\n",
       "  '\\n',\n",
       "  '            all_objects = list(\\n',\n",
       "  '                itertools.chain.from_iterable([trk._data for trk in data])\\n',\n",
       "  '            )\\n',\n",
       "  '\\n',\n",
       "  '            objects = [obj for obj in all_objects if not obj.dummy]\\n',\n",
       "  '            dummies = [obj for obj in all_objects if obj.dummy]\\n',\n",
       "  '\\n',\n",
       "  '            # renumber the object ID so that they can be stored in a contiguous\\n',\n",
       "  '            # array and indexed by row - this may not be necessary for most\\n',\n",
       "  '            # datasets, but is here just in case\\n',\n",
       "  '            for idx, obj in enumerate(objects):\\n',\n",
       "  '                obj.ID = idx\\n',\n",
       "  '\\n',\n",
       "  '            for idx, dummy in enumerate(dummies):\\n',\n",
       "  '                dummy.ID = -(idx + 1)\\n',\n",
       "  '\\n',\n",
       "  '            refs = [trk.refs for trk in data]\\n',\n",
       "  '            lbep_table = utils._lbep_table(data)\\n',\n",
       "  '            fate_table = np.stack([t.fate.value for t in data], axis=0)\\n',\n",
       "  '\\n',\n",
       "  '            if \"objects\" not in self._hdf:\\n',\n",
       "  '                self.write_objects(objects)\\n',\n",
       "  '\\n',\n",
       "  '        elif hasattr(data, \"tracks\"):\\n',\n",
       "  '            refs = data.refs\\n',\n",
       "  '            dummies = data.dummies\\n',\n",
       "  '            lbep_table = data.LBEP\\n',\n",
       "  '            fate_table = np.stack([t.fate.value for t in data.tracks], axis=0)\\n',\n",
       "  '        else:\\n',\n",
       "  '            raise ValueError(f\"Data of type {type(data)} not supported.\")\\n',\n",
       "  '\\n',\n",
       "  '        if not refs:\\n',\n",
       "  '            logger.error(f\"No tracks found when exporting to: {self.filename}\")\\n',\n",
       "  '            return\\n',\n",
       "  '\\n',\n",
       "  '        # sanity check\\n',\n",
       "  '        assert lbep_table.shape[0] == len(refs)\\n',\n",
       "  '\\n',\n",
       "  '        logger.info(f\"Writing tracks/{self.object_type}\")\\n',\n",
       "  '        hdf_tracks = np.concatenate(refs, axis=0)\\n',\n",
       "  '\\n',\n",
       "  '        hdf_frame_map = np.zeros((len(refs), 2), dtype=np.int32)\\n',\n",
       "  '        for i, track in enumerate(refs):\\n',\n",
       "  '            if i > 0:\\n',\n",
       "  '                offset = hdf_frame_map[i - 1, 1]\\n',\n",
       "  '            else:\\n',\n",
       "  '                offset = 0\\n',\n",
       "  '            hdf_frame_map[i, :] = np.array([0, len(track)]) + offset\\n',\n",
       "  '\\n',\n",
       "  '        if \"tracks\" not in self._hdf:\\n',\n",
       "  '            self._hdf.create_group(\"tracks\")\\n',\n",
       "  '\\n',\n",
       "  '        if self.object_type in self._hdf[\"tracks\"]:\\n',\n",
       "  '            logger.warning(f\"Removing tracks/{self.object_type}.\")\\n',\n",
       "  '            del self._hdf[\"tracks\"][self.object_type]\\n',\n",
       "  '\\n',\n",
       "  '        grp = self._hdf[\"tracks\"].create_group(self.object_type)\\n',\n",
       "  '        grp.create_dataset(\"tracks\", data=hdf_tracks, dtype=\"int32\")\\n',\n",
       "  '        grp.create_dataset(\"map\", data=hdf_frame_map, dtype=\"uint32\")\\n',\n",
       "  '\\n',\n",
       "  '        # if we have used the f_expr we can save it as an attribute here\\n',\n",
       "  '        if f_expr is not None and isinstance(f_expr, str):\\n',\n",
       "  '            grp.attrs[\"f_expr\"] = f_expr\\n',\n",
       "  '\\n',\n",
       "  '        # also save the version number as an attribute\\n',\n",
       "  '        grp.attrs[\"version\"] = constants.get_version()\\n',\n",
       "  '\\n',\n",
       "  '        # write out dummies\\n',\n",
       "  '        if dummies:\\n',\n",
       "  '            logger.info(f\"Writing dummies/{self.object_type}\")\\n',\n",
       "  '            o = self.object_types.index(self.object_type) + 1\\n',\n",
       "  '            txyz = np.stack([[d.t, d.x, d.y, d.z, o] for d in dummies], axis=0)\\n',\n",
       "  '            grp.create_dataset(\"dummies\", data=txyz, dtype=\"float32\")\\n',\n",
       "  '\\n',\n",
       "  '        # write out the LBEP table\\n',\n",
       "  '        logger.info(f\"Writing LBEP/{self.object_type}\")\\n',\n",
       "  '        grp.create_dataset(\"LBEPR\", data=lbep_table, dtype=\"int32\")\\n',\n",
       "  '\\n',\n",
       "  '        # write out cell fates\\n',\n",
       "  '        logger.info(f\"Writing fates/{self.object_type}\")\\n',\n",
       "  '        grp.create_dataset(\"fates\", data=fate_table, dtype=\"int32\")\\n',\n",
       "  '\\n',\n",
       "  '    @property  # type: ignore\\n',\n",
       "  '    @h5check_property_exists(\"tracks\")\\n',\n",
       "  '    def lbep(self) -> np.ndarray:\\n',\n",
       "  '        \"\"\"Return the LBEP data.\"\"\"\\n',\n",
       "  '        logger.info(f\"Loading LBEP/{self.object_type}\")\\n',\n",
       "  '        return self._hdf[\"tracks\"][self.object_type][\"LBEPR\"][:]\\n',\n",
       "  '\\n',\n",
       "  '\\n',\n",
       "  'if __name__ == \"__main__\":\\n',\n",
       "  '    pass\\n'],\n",
       " 0)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import inspect\n",
    "inspect.getsourcelines(btrack.dataio) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2012b182",
   "metadata": {},
   "source": [
    "#### testing unique fn "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "06aba2df",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'co-culture_iVECs+iAT2AT1_Folder_20220808_A2-A5_analysis_20221125_DAPI-SPC-PDPN-ZO1__20220808_kolf-WT_co-culture_20x_A2-A5_Multichannel Z-Stack_20220808_60__tracks.hdf5'"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tracks_fn.split('Light microscopy/')[-1].replace('/', '_')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f99c13f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "ce5a5378",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/run/user/30046150/gvfs/smb-share:server=data.thecrick.org,share=lab-gutierrezm/home/shared/Lung on chip/Light microscopy/co-culture/iVECs+iAT2AT1/Folder_20220808/A2-A5/analysis_20221125/DAPI-SPC-PDPN-ZO1/_20220808_kolf-WT_co-culture_20x_A2-A5_Multichannel Z-Stack_20220808_60_/tracks.hdf5'"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tracks_fn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8f65025",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6ca35232",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_tracks_fn = '/run/user/30046150/gvfs/smb-share:server=data.thecrick.org,share=lab-gutierrezm/home/users/dayn/test_tracks_output/tracks.hdf5'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7e2ecedb",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO][2023/01/10 10:38:28 AM] Opening HDF file: /run/user/30046150/gvfs/smb-share:server=data.thecrick.org,share=lab-gutierrezm/home/users/dayn/test_tracks_output/tracks.hdf5...\n",
      "[INFO][2023/01/10 10:38:28 AM] Writing objects/obj_type_1\n",
      "[INFO][2023/01/10 10:38:28 AM] Writing labels/obj_type_1\n",
      "[INFO][2023/01/10 10:38:29 AM] Loading objects/obj_type_1 (29045, 5) (29045 filtered: None)\n",
      "[INFO][2023/01/10 10:38:29 AM] Writing properties/obj_type_1/axis_major_length (29045,)\n",
      "[INFO][2023/01/10 10:38:29 AM] Closing HDF file: /run/user/30046150/gvfs/smb-share:server=data.thecrick.org,share=lab-gutierrezm/home/users/dayn/test_tracks_output/tracks.hdf5\n"
     ]
    },
    {
     "ename": "OSError",
     "evalue": "[Errno 95] Unable to extend file properly, errno = 95, error message = 'operation not supported' (file write failed: time = Tue Jan 10 10:38:29 2023\n, filename = '/run/user/30046150/gvfs/smb-share:server=data.thecrick.org,share=lab-gutierrezm/home/users/dayn/test_tracks_output/tracks.hdf5', file descriptor = 59, errno = 95, error message = 'Operation not supported', buf = 0x890b058, total write size = 3368, bytes this sub-write = 3368, bytes actually written = 18446744073709551615, offset = 0)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "Input \u001b[0;32mIn [12]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m btrack\u001b[38;5;241m.\u001b[39mdataio\u001b[38;5;241m.\u001b[39mHDF5FileHandler(test_tracks_fn, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mw\u001b[39m\u001b[38;5;124m\"\u001b[39m, obj_type\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mobj_type_1\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m hdf:\n\u001b[0;32m----> 2\u001b[0m     \u001b[43mhdf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwrite_tracks\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtracks\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/analysis/BayesianTracker_/btrack/dataio.py:752\u001b[0m, in \u001b[0;36mHDF5FileHandler.write_tracks\u001b[0;34m(self, data, f_expr)\u001b[0m\n\u001b[1;32m    751\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mobjects\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_hdf:\n\u001b[0;32m--> 752\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwrite_objects\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobjects\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    754\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(data, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtracks\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n",
      "File \u001b[0;32m~/analysis/BayesianTracker_/btrack/dataio.py:579\u001b[0m, in \u001b[0;36mHDF5FileHandler.write_objects\u001b[0;34m(self, data)\u001b[0m\n\u001b[1;32m    578\u001b[0m \u001b[38;5;66;03m# finally, write any properties\u001b[39;00m\n\u001b[0;32m--> 579\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwrite_properties\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprops\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/analysis/BayesianTracker_/btrack/dataio.py:286\u001b[0m, in \u001b[0;36mh5check_property_exists.<locals>.func.<locals>.wrapped_handler_property\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    285\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m--> 286\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/analysis/BayesianTracker_/btrack/dataio.py:640\u001b[0m, in \u001b[0;36mHDF5FileHandler.write_properties\u001b[0;34m(self, data, allow_overwrite)\u001b[0m\n\u001b[1;32m    637\u001b[0m logger\u001b[38;5;241m.\u001b[39minfo(\n\u001b[1;32m    638\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWriting properties/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mobject_type\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mkey\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mvalues\u001b[38;5;241m.\u001b[39mshape\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    639\u001b[0m )\n\u001b[0;32m--> 640\u001b[0m \u001b[43mprops_grp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcreate_dataset\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdata\u001b[49m\u001b[43m[\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mfloat32\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/btrack_test/lib/python3.9/site-packages/h5py/_hl/group.py:161\u001b[0m, in \u001b[0;36mGroup.create_dataset\u001b[0;34m(self, name, shape, dtype, data, **kwds)\u001b[0m\n\u001b[1;32m    159\u001b[0m         group \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrequire_group(parent_path)\n\u001b[0;32m--> 161\u001b[0m dsid \u001b[38;5;241m=\u001b[39m \u001b[43mdataset\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmake_new_dset\u001b[49m\u001b[43m(\u001b[49m\u001b[43mgroup\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mshape\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    162\u001b[0m dset \u001b[38;5;241m=\u001b[39m dataset\u001b[38;5;241m.\u001b[39mDataset(dsid)\n",
      "File \u001b[0;32m~/miniconda3/envs/btrack_test/lib/python3.9/site-packages/h5py/_hl/dataset.py:159\u001b[0m, in \u001b[0;36mmake_new_dset\u001b[0;34m(parent, shape, dtype, data, name, chunks, compression, shuffle, fletcher32, maxshape, compression_opts, fillvalue, scaleoffset, track_times, external, track_order, dcpl, dapl, efile_prefix, virtual_prefix, allow_unknown_filter)\u001b[0m\n\u001b[1;32m    158\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (data \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;129;01mand\u001b[39;00m (\u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data, Empty)):\n\u001b[0;32m--> 159\u001b[0m     \u001b[43mdset_id\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwrite\u001b[49m\u001b[43m(\u001b[49m\u001b[43mh5s\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mALL\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mh5s\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mALL\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    161\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m dset_id\n",
      "File \u001b[0;32mh5py/_objects.pyx:54\u001b[0m, in \u001b[0;36mh5py._objects.with_phil.wrapper\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mh5py/_objects.pyx:55\u001b[0m, in \u001b[0;36mh5py._objects.with_phil.wrapper\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mh5py/h5d.pyx:232\u001b[0m, in \u001b[0;36mh5py.h5d.DatasetID.write\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mh5py/_proxy.pyx:114\u001b[0m, in \u001b[0;36mh5py._proxy.dset_rw\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mOSError\u001b[0m: [Errno 95] Can't write data (file write failed: time = Tue Jan 10 10:38:29 2023\n, filename = '/run/user/30046150/gvfs/smb-share:server=data.thecrick.org,share=lab-gutierrezm/home/users/dayn/test_tracks_output/tracks.hdf5', file descriptor = 59, errno = 95, error message = 'Operation not supported', buf = 0x674fc00, total write size = 116180, bytes this sub-write = 116180, bytes actually written = 18446744073709551615, offset = 0)",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "Input \u001b[0;32mIn [12]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m btrack\u001b[38;5;241m.\u001b[39mdataio\u001b[38;5;241m.\u001b[39mHDF5FileHandler(test_tracks_fn, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mw\u001b[39m\u001b[38;5;124m\"\u001b[39m, obj_type\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mobj_type_1\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m hdf:\n\u001b[0;32m----> 2\u001b[0m     hdf\u001b[38;5;241m.\u001b[39mwrite_tracks(tracks)\n",
      "File \u001b[0;32m~/analysis/BayesianTracker_/btrack/dataio.py:392\u001b[0m, in \u001b[0;36mHDF5FileHandler.__exit__\u001b[0;34m(self, exc_type, exc_value, traceback)\u001b[0m\n\u001b[1;32m    391\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__exit__\u001b[39m(\u001b[38;5;28mself\u001b[39m, exc_type, exc_value, traceback):\n\u001b[0;32m--> 392\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mclose\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/analysis/BayesianTracker_/btrack/dataio.py:398\u001b[0m, in \u001b[0;36mHDF5FileHandler.close\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    396\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[1;32m    397\u001b[0m logger\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mClosing HDF file: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfilename\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 398\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_hdf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mclose\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/btrack_test/lib/python3.9/site-packages/h5py/_hl/files.py:552\u001b[0m, in \u001b[0;36mFile.close\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    546\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mid\u001b[38;5;241m.\u001b[39mvalid:\n\u001b[1;32m    547\u001b[0m     \u001b[38;5;66;03m# We have to explicitly murder all open objects related to the file\u001b[39;00m\n\u001b[1;32m    548\u001b[0m \n\u001b[1;32m    549\u001b[0m     \u001b[38;5;66;03m# Close file-resident objects first, then the files.\u001b[39;00m\n\u001b[1;32m    550\u001b[0m     \u001b[38;5;66;03m# Otherwise we get errors in MPI mode.\u001b[39;00m\n\u001b[1;32m    551\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mid\u001b[38;5;241m.\u001b[39m_close_open_objects(h5f\u001b[38;5;241m.\u001b[39mOBJ_LOCAL \u001b[38;5;241m|\u001b[39m \u001b[38;5;241m~\u001b[39mh5f\u001b[38;5;241m.\u001b[39mOBJ_FILE)\n\u001b[0;32m--> 552\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mid\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_close_open_objects\u001b[49m\u001b[43m(\u001b[49m\u001b[43mh5f\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mOBJ_LOCAL\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m|\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mh5f\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mOBJ_FILE\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    554\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mid\u001b[38;5;241m.\u001b[39mclose()\n\u001b[1;32m    555\u001b[0m     _objects\u001b[38;5;241m.\u001b[39mnonlocal_close()\n",
      "File \u001b[0;32mh5py/_objects.pyx:54\u001b[0m, in \u001b[0;36mh5py._objects.with_phil.wrapper\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mh5py/_objects.pyx:55\u001b[0m, in \u001b[0;36mh5py._objects.with_phil.wrapper\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mh5py/h5f.pyx:360\u001b[0m, in \u001b[0;36mh5py.h5f.FileID._close_open_objects\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mOSError\u001b[0m: [Errno 95] Unable to extend file properly, errno = 95, error message = 'operation not supported' (file write failed: time = Tue Jan 10 10:38:29 2023\n, filename = '/run/user/30046150/gvfs/smb-share:server=data.thecrick.org,share=lab-gutierrezm/home/users/dayn/test_tracks_output/tracks.hdf5', file descriptor = 59, errno = 95, error message = 'Operation not supported', buf = 0x890b058, total write size = 3368, bytes this sub-write = 3368, bytes actually written = 18446744073709551615, offset = 0)"
     ]
    }
   ],
   "source": [
    "with btrack.dataio.HDF5FileHandler(test_tracks_fn, \"w\", obj_type=\"obj_type_1\") as hdf:\n",
    "    hdf.write_tracks(tracks)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54a9d97b",
   "metadata": {},
   "source": [
    "## Checking tracks prior to plotting single cell info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c21df49d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import napari"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "335f35ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "viewer = napari.Viewer()\n",
    "\n",
    "viewer.add_image(ch0, colormap='blue', blending = 'additive')\n",
    "viewer.add_image(ch3, colormap= 'gray', blending = 'additive')\n",
    "viewer.add_image(ch1, colormap='red', blending = 'additive')\n",
    "viewer.add_image(ch2, colormap= 'green', blending = 'additive')\n",
    "\n",
    "viewer.add_labels(mask_stack)\n",
    "\n",
    "viewer.add_tracks(ch1_data)\n",
    "viewer.add_tracks(ch2_data)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "btrack_test",
   "language": "python",
   "name": "btrack_test"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
